@article{baker2016designing,
	title={Designing neural network architectures using reinforcement learning},
	author={Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
	journal={arXiv preprint arXiv:1611.02167},
	year={2016}
}

@inproceedings{bergstra2013hyperopt,
	title={Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms},
	author={Bergstra, James and Yamins, Dan and Cox, David D and others},
	booktitle={Proceedings of the 12th Python in science conference},
	volume={13},
	pages={20},
	year={2013},
	organization={Citeseer}
}

@article{bischl2016aslib,
	title={Aslib: A benchmark library for algorithm selection},
	author={Bischl, Bernd and Kerschke, Pascal and Kotthoff, Lars and Lindauer, Marius and Malitsky, Yuri and Fr{\'e}chette, Alexandre and Hoos, Holger and Hutter, Frank and Leyton-Brown, Kevin and Tierney, Kevin and others},
	journal={Artificial Intelligence},
	volume={237},
	pages={41--58},
	year={2016},
	publisher={Elsevier}
}


@inproceedings{chen2018autostacker,
	title={Autostacker: A compositional evolutionary learning system},
	author={Chen, Boyuan and Wu, Harvey and Mo, Warren and Chattopadhyay, Ishanu and Lipson, Hod},
	booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
	pages={402--409},
	year={2018}
}

@inproceedings{de2017recipe,
	title={RECIPE: a grammar-based framework for automatically evolving classification pipelines},
	author={de S{\'a}, Alex GC and Pinto, Walter Jos{\'e} GS and Oliveira, Luiz Otavio VB and Pappa, Gisele L},
	booktitle={European Conference on Genetic Programming},
	pages={246--261},
	year={2017},
	organization={Springer}
}

@inproceedings{de2018automated,
	title={Automated selection and configuration of multi-label classification algorithms with grammar-based genetic programming},
	author={de S{\'a}, Alex GC and Freitas, Alex A and Pappa, Gisele L},
	booktitle={International Conference on Parallel Problem Solving from Nature},
	pages={308--320},
	year={2018},
	organization={Springer}
}

@article{elshawi2019automated,
	title={Automated machine learning: State-of-the-art and open challenges},
	author={Elshawi, Radwa and Maher, Mohamed and Sakr, Sherif},
	journal={arXiv preprint arXiv:1906.02287},
	year={2019}
}

@inproceedings{jin2019auto,
	title={Auto-keras: An efficient neural architecture search system},
	author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
	booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages={1946--1956},
	year={2019}
}

@inproceedings{komer2014hyperopt,
	title={Hyperopt-sklearn: automatic hyperparameter configuration for scikit-learn},
	author={Komer, Brent and Bergstra, James and Eliasmith, Chris},
	booktitle={ICML workshop on AutoML},
	volume={9},
	pages={50},
	year={2014},
	organization={Citeseer}
}

@inproceedings{maher2019smartml,
	title={Smartml: A meta learning-based framework for automated selection and hyperparameter tuning for machine learning algorithms},
	author={Maher, Mohamed and Sakr, Sherif},
	booktitle={EDBT: 22nd International Conference on Extending Database Technology},
	year={2019}
}

@inproceedings{mendoza2016towards,
	title={Towards automatically-tuned neural networks},
	author={Mendoza, Hector and Klein, Aaron and Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
	booktitle={Workshop on Automatic Machine Learning},
	pages={58--65},
	year={2016},
	organization={PMLR}
}

@article{mohr2018ml,
	title={ML-Plan: Automated machine learning via hierarchical planning},
	author={Mohr, Felix and Wever, Marcel and H{\"u}llermeier, Eyke},
	journal={Machine Learning},
	volume={107},
	number={8},
	pages={1495--1515},
	year={2018},
	publisher={Springer}
}

@inproceedings{fusi2018advances,
	author = {Fusi, Nicolo and Sheth, Rishit and Elibol, Melih},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Probabilistic Matrix Factorization for Automated Machine Learning},
	url = {https://proceedings.neurips.cc/paper/2018/file/b59a51a3c0bf9c5228fde841714f523a-Paper.pdf},
	volume = {31},
	year = {2018}
}


@article{yang2018oboe,
	author    = {Chengrun Yang and
	Yuji Akimoto and
	Dae Won Kim and
	Madeleine Udell},
	title     = {{OBOE:} Collaborative Filtering for AutoML Initialization},
	journal   = {CoRR},
	volume    = {abs/1808.03233},
	year      = {2018},
	url       = {http://arxiv.org/abs/1808.03233},
	archivePrefix = {arXiv},
	eprint    = {1808.03233},
	timestamp = {Sun, 02 Sep 2018 15:01:57 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1808-03233.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{olson2016evaluation,
	title={Evaluation of a tree-based pipeline optimization tool for automating data science},
	author={Olson, Randal S and Bartley, Nathan and Urbanowicz, Ryan J and Moore, Jason H},
	booktitle={Proceedings of the genetic and evolutionary computation conference 2016},
	pages={485--492},
	year={2016}
}

@inproceedings{pham2018efficient,
	title={Efficient neural architecture search via parameters sharing},
	author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
	booktitle={International Conference on Machine Learning},
	pages={4095--4104},
	year={2018},
	organization={PMLR}
}

@article{rakotoarison2019automated,
	title={Automated machine learning with monte-carlo tree search},
	author={Rakotoarison, Herilalaina and Schoenauer, Marc and Sebag, Mich{\`e}le},
	journal={arXiv preprint arXiv:1906.00170},
	year={2019}
}

@inproceedings{evans2020adaptive,
	title={An adaptive and near parameter-free evolutionary computation approach towards true automation in automl},
	author={Evans, Benjamin and Xue, Bing and Zhang, Mengjie},
	booktitle={2020 IEEE Congress on Evolutionary Computation (CEC)},
	pages={1--8},
	year={2020},
	organization={IEEE}
}

@inproceedings{shang2019democratizing,
	title={Democratizing data science through interactive curation of ml pipelines},
	author={Shang, Zeyuan and Zgraggen, Emanuel and Buratti, Benedetto and Kossmann, Ferdinand and Eichmann, Philipp and Chung, Yeounoh and Binnig, Carsten and Upfal, Eli and Kraska, Tim},
	booktitle={Proceedings of the 2019 International Conference on Management of Data},
	pages={1171--1188},
	year={2019}
}

@inproceedings{thornton2013auto,
	title={Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms},
	author={Thornton, Chris and Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
	booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages={847--855},
	year={2013}
}

@article{wang2018rafiki,
	title={Rafiki: Machine learning as an analytics service system},
	author={Wang, Wei and Wang, Sheng and Gao, Jinyang and Zhang, Meihui and Chen, Gang and Ng, Teck Khim and Ooi, Beng Chin},
	journal={arXiv preprint arXiv:1804.06087},
	year={2018}
}


@article{wang2021flaml,
	title={FLAML: A Fast and Lightweight AutoML Library},
	author={Wang, Chi and Wu, Qingyun and Weimer, Markus and Zhu, Erkang},
	journal={Proceedings of Machine Learning and Systems},
	volume={3},
	year={2021}
}

@article{zoph2016neural,
	title={Neural architecture search with reinforcement learning},
	author={Zoph, Barret and Le, Quoc V},
	journal={arXiv preprint arXiv:1611.01578},
	year={2016}
}

@inproceedings{swearingen2017atm,
	title={ATM: A distributed, collaborative, scalable system for automated machine learning},
	author={Swearingen, Thomas and Drevo, Will and Cyphers, Bennett and Cuesta-Infante, Alfredo and Ross, Arun and Veeramachaneni, Kalyan},
	booktitle={2017 IEEE International Conference on Big Data (Big Data)},
	pages={151--162},
	year={2017},
	organization={IEEE}
}

@inproceedings{yang2020automl,
	title={Automl pipeline selection: Efficiently navigating the combinatorial space},
	author={Yang, Chengrun and Fan, Jicong and Wu, Ziyang and Udell, Madeleine},
	booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages={1446--1456},
	year={2020}
}

@article{zimmer2021auto,
	title={Auto-Pytorch: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL},
	author={Zimmer, Lucas and Lindauer, Marius and Hutter, Frank},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	year={2021},
	publisher={IEEE}
}

@inproceedings{drori2018alphad3m,
	title={AlphaD3M: Machine learning pipeline synthesis},
	author={Drori, Iddo and Krishnamurthy, Yamuna and Rampin, Remi and Louren{\c{c}}o, Raoni and One, Jorge and Cho, Kyunghyun and Silva, Claudio and Freire, Juliana},
	booktitle={AutoML Workshop at ICML},
	year={2018}
}

@inproceedings{fuerer2015efficient,
	author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Efficient and Robust Automated Machine Learning},
	url = {https://proceedings.neurips.cc/paper/2015/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf},
	volume = {28},
	year = {2015}
}

@Inbook{olson2019tpot,
	author="Olson, Randal S.
	and Moore, Jason H.",
	editor="Hutter, Frank
	and Kotthoff, Lars
	and Vanschoren, Joaquin",
	title="TPOT: A Tree-Based Pipeline Optimization Tool for Automating Machine Learning",
	bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
	year="2019",
	publisher="Springer International Publishing",
	address="Cham",
	pages="151--160",
	abstract="As data science becomes increasingly mainstream, there will be an ever-growing demand for data science tools that are more accessible, flexible, and scalable. In response to this demand, automated machine learning (AutoML) researchers have begun building systems that automate the process of designing and optimizing machine learning pipelines. In this chapter we present TPOT v0.3, an open source genetic programming-based AutoML system that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification task. We benchmark TPOT on a series of 150 supervised classification tasks and find that it significantly outperforms a basic machine learning analysis in 21 of them, while experiencing minimal degradation in accuracy on 4 of the benchmarks---all without any domain knowledge nor human input. As such, genetic programming-based AutoML systems show considerable promise in the AutoML domain.",
	isbn="978-3-030-05318-5",
	doi="10.1007/978-3-030-05318-5_8",
	url="https://doi.org/10.1007/978-3-030-05318-5_8"
}

@inproceedings{chen2018autostacker,
	author = {Chen, Boyuan and Wu, Harvey and Mo, Warren and Chattopadhyay, Ishanu and Lipson, Hod},
	title = {Autostacker: A Compositional Evolutionary Learning System},
	year = {2018},
	isbn = {9781450356183},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3205455.3205586},
	doi = {10.1145/3205455.3205586},
	abstract = {In this work, an automatic machine learning (AutoML) modeling architecture called
	Autostacker is introduced. Autostacker combines an innovative hierarchical stacking
	architecture and an evolutionary algorithm (EA) to perform efficient parameter search
	without the need for prior domain knowledge about the data or feature preprocessing.
	Using EA, Autostacker quickly evolves candidate pipelines with high predictive accuracy.
	These pipelines can be used in their given form, or serve as a starting point for
	further augmentation and refinement by human experts. Autostacker finds innovative
	machine learning model combinations and structures, rather than selecting a single
	model and optimizing its hyperparameters. When its performance on fifteen datasets
	is compared with that of other AutoML systems, Autostacker produces superior or competitive
	results in terms of both test accuracy and time cost.},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	pages = {402–409},
	numpages = {8},
	keywords = {autoML, evolutionary machine learning, machine learning},
	location = {Kyoto, Japan},
	series = {GECCO '18}
}

@inproceedings{real2020automl,
	title={Automl-zero: Evolving machine learning algorithms from scratch},
	author={Real, Esteban and Liang, Chen and So, David and Le, Quoc},
	booktitle={International Conference on Machine Learning},
	pages={8007--8019},
	year={2020},
	organization={PMLR}
}

@article{NSGA-II,
	author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
	title = {A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II},
	year = {2002},
	issue_date = {April 2002},
	publisher = {IEEE Press},
	volume = {6},
	number = {2},
	issn = {1089-778X},
	url = {https://doi.org/10.1109/4235.996017},
	doi = {10.1109/4235.996017},
	abstract = {Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and
	sharing have been criticized mainly for: (1) their O(MN3) computational complexity
	(where M is the number of objectives and N is the population size); (2) their non-elitism
	approach; and (3) the need to specify a sharing parameter. In this paper, we suggest
	a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic
	Algorithm II), which alleviates all of the above three difficulties. Specifically,
	a fast non-dominated sorting approach with O(MN2) computational complexity is presented.
	Also, a selection operator is presented that creates a mating pool by combining the
	parent and offspring populations and selecting the best N solutions (with respect
	to fitness and spread). Simulation results on difficult test problems show that NSGA-II
	is able, for most problems, to find a much better spread of solutions and better convergence
	near the true Pareto-optimal front compared to the Pareto-archived evolution strategy
	and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay
	special attention to creating a diverse Pareto-optimal front. Moreover, we modify
	the definition of dominance in order to solve constrained multi-objective problems
	efficiently. Simulation results of the constrained NSGA-II on a number of test problems,
	including a five-objective, seven-constraint nonlinear problem, are compared with
	another constrained multi-objective optimizer, and the much better performance of
	NSGA-II is observed},
	journal = {Trans. Evol. Comp},
	month = apr,
	pages = {182–197},
	numpages = {16}
}

@article{alphazero,
	author    = {David Silver and
	Thomas Hubert and
	Julian Schrittwieser and
	Ioannis Antonoglou and
	Matthew Lai and
	Arthur Guez and
	Marc Lanctot and
	Laurent Sifre and
	Dharshan Kumaran and
	Thore Graepel and
	Timothy P. Lillicrap and
	Karen Simonyan and
	Demis Hassabis},
	title     = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
	Learning Algorithm},
	journal   = {CoRR},
	volume    = {abs/1712.01815},
	year      = {2017},
	url       = {http://arxiv.org/abs/1712.01815},
	eprinttype = {arXiv},
	eprint    = {1712.01815},
	timestamp = {Mon, 13 Aug 2018 16:46:01 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{OracleAutoML,
	author = {Yakovlev, Anatoly and Moghadam, Hesam Fathi and Moharrer, Ali and Cai, Jingxiao and Chavoshi, Nikan and Varadarajan, Venkatanathan and Agrawal, Sandeep R. and Idicula, Sam and Karnagel, Tomas and Jinturkar, Sanjay and Agarwal, Nipun},
	title = {Oracle AutoML: A Fast and Predictive AutoML Pipeline},
	year = {2020},
	issue_date = {August 2020},
	publisher = {VLDB Endowment},
	volume = {13},
	number = {12},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3415478.3415542},
	doi = {10.14778/3415478.3415542},
	abstract = {Machine learning (ML) is at the forefront of the rising popularity of data-driven
	software applications. The resulting rapid proliferation of ML technology, explosive
	data growth, and shortage of data science expertise have caused the industry to face
	increasingly challenging demands to keep up with fast-paced develop-and-deploy model
	lifecycles. Recent academic and industrial research efforts have started to address
	this problem through automated machine learning (AutoML) pipelines and have focused
	on model performance as the first-order design objective. We present Oracle AutoML,
	a novel iteration-free AutoML pipeline designed to not only provide accurate models,
	but also in a shorter runtime. We are able to achieve these objectives by eliminating
	the need to continuously iterate over various pipeline configurations. In our feed-forward
	approach, each pipeline stage makes decisions based on metalearned proxy models that
	can predict candidate pipeline configuration performances before building the full
	final model. Our approach, which builds and tunes only the best candidate pipeline,
	achieves better scores at a fraction of the time compared to state-of-the-art open
	source AutoML tools, such as H2O and Auto-sklearn. This makes Oracle AutoML a prime
	candidate for addressing current industry challenges.},
	journal = {Proc. VLDB Endow.},
	month = aug,
	pages = {3166–3180},
	numpages = {15}
}

@article{sun2013pairwise,
	title={Pairwise meta-rules for better meta-learning-based algorithm ranking},
	author={Sun, Quan and Pfahringer, Bernhard},
	journal={Machine learning},
	volume={93},
	number={1},
	pages={141--161},
	year={2013},
	publisher={Springer}
}

@inproceedings{Pinto2016TowardsAG,
	title={Towards Automatic Generation of Metafeatures},
	author={F{\'a}bio Pinto and C. Soares and Jo{\~a}o Mendes-Moreira},
	booktitle={PAKDD},
	year={2016}
}

@inproceedings{pinto2014framework,
	author = {Pinto, F\'{a}bio and Soares, Carlos and Mendes-Moreira, Jo\~{a}o},
	title = {A Framework to Decompose and Develop Metafeatures},
	year = {2014},
	isbn = {16130073},
	publisher = {CEUR-WS.org},
	address = {Aachen, DEU},
	abstract = {This paper proposes a framework to decompose and develop metafeatures for Metalearning
	(MtL) problems. Several metafeatures (also known as data characteristics) are proposed
	in the literature for a wide range of problems. Since MtL applicability is very general
	but problem dependent, researchers focus on generating specific and yet informative
	metafeatures for each problem. This process is carried without any sort of conceptual
	framework. We believe that such framework would open new horizons on the development
	of metafeatures and also aid the process of understanding the metafeatures already
	proposed in the state-of-the-art. We propose a framework with the aim of fill that
	gap and we show its applicability in a scenario of algorithm recommendation for regression
	problems.},
	booktitle = {Proceedings of the 2014 International Conference on Meta-Learning and Algorithm Selection - Volume 1201},
	pages = {32–36},
	numpages = {5},
	location = {Prague, Czech Republic},
	series = {MLAS'14}
}

@article{bilalli2017predictive,
	author = {Bilalli, Besim and Abell\'{o}, Alberto and Aluja-Banet, Tom\'{\i}s},
	title = {On the Predictive Power of Meta-Features in OpenML},
	year = {2017},
	issue_date = {20 12 2017},
	publisher = {Walter de Gruyter &amp; Co.},
	address = {USA},
	volume = {27},
	number = {4},
	issn = {1641-876X},
	url = {https://doi.org/10.1515/amcs-2017-0048},
	doi = {10.1515/amcs-2017-0048},
	abstract = {Abstract The demand for performing data analysis is steadily rising. As a consequence,
	people of different profiles i.e., nonexperienced users have started to analyze their
	data. However, this is challenging for them. A key step that poses difficulties and
	determines the success of the analysis is data mining model/algorithm selection problem.
	Meta-learning is a technique used for assisting non-expert users in this step. The
	effectiveness of meta-learning is, however, largely dependent on the description/characterization
	of datasets i.e., meta-features used for meta-learning. There is a need for improving
	the effectiveness of meta-learning by identifying and designing more predictive meta-features.
	In this work, we use a method from exploratory factor analysis to study the predictive
	power of different meta-features collected in OpenML, which is a collaborative machine
	learning platform that is designed to store and organize meta-data about datasets,
	data mining algorithms, models and their evaluations. We first use the method to extract
	latent features, which are abstract concepts that group together meta-features with
	common characteristics. Then, we study and visualize the relationship of the latent
	features with three different performance measures of four classification algorithms
	on hundreds of datasets available in OpenML, and we select the latent features with
	the highest predictive power. Finally, we use the selected latent features to perform
	meta-learning and we show that our method improves the meta-learning process. Furthermore,
	we design an easy to use application for retrieving different meta-data from OpenML
	as the biggest source of data in this domain.},
	journal = {Int. J. Appl. Math. Comput. Sci.},
	month = dec,
	pages = {697–712},
	numpages = {16},
	keywords = {feature extraction, meta-learning, feature selection}
}

@article{Rivolli2018TowardsRE,
	title={Towards Reproducible Empirical Research in Meta-Learning},
	author={Adriano Rivolli and L. P. F. Garcia and Carlos Soares and J. Vanschoren and A. Carvalho},
	journal={ArXiv},
	year={2018},
	volume={abs/1808.10406}
}