% !TeX spellcheck = es_ES
\chapter{Resultados Experimentales}\label{chapter:results}

% - Hacer un overview del capítulo
El objetivo de este capítulo es evaluar los métodos propuestos de meta-learning en la tarea de la creación de un ranking de algoritmos y en su utilización como inicialización de un intenso proceso de optimización. El estudio realizado se enfoca solo en el rendimiento del algoritmo de meta-learning en los datasets de clasificación. Para ser capaces de sacar conclusiones estadísticamente significativas, se eligieron 305 datasets, en dependencia de los recursos computacionales y el tiempo disponible. En este capítulo se explica el método seguido para la obtención de dichos datasets, la distribución de sus características y el procedimiento seguido para la formación de los conjuntos de entrenamiento y de prueba utilizados en la experimentación (Sección~\ref{sec:datasets}). Luego, se describe la metodología seguida para la generación de los flujos de algoritmos que son usados como experiencia pasada por los métodos de meta-learning propuestos (Sección \ref{sec:flujos}). Primero se describe brevemente las configuraciones utilizadas en los modelos implementados (Sección~\ref{sec:comparacion}), y después 
%se realiza una evaluación de la precisión del ranking para evaluar la eficiencia de los modelos en cuanto a la creación de rankings de algoritmos (Sección \ref{subsec:ranking}). Posteriormente, 
se exponen los resultados obtenidos al añadir las configuraciones recomendadas por los métodos propuestos en la inicialización de la optimización de AutoGOAL, comparándolas con la versión de AutoGOAL que no tiene meta-learning (Sección~\ref{subsec:resultados}). Se evalúan varios aspectos, como la cantidad de flujos inválidos generados durante la optimización y los resultados finales obtenidos para cada una de las estrategias. Por último, se presenta una discusión sobre los resultados obtenidos y se mencionan algunas limitaciones del método propuesto (Sección~\ref{sec:discusion}).


\section{Datasets}\label{sec:datasets}

%- Hablar un poco de openml, que fue usado para extraer los datasets
%
%- Hablar de cuales fueron los datasets seleccionados y cómo fueron seleccionados
%
%- Características de los datasets seleccionados
%
%- Hablar de cómo fueron divididos los datasets para la experimentación

Para los experimentos realizados se extrajeron datasets de clasificación de OpenML \cite{vanschoren2014openml}. OpenML es una plataforma de código abierto desarrollada con el objetivo de permitir a los investigadores compartir sus datasets, implementaciones y experimentos de una forma tal que ellas puedan ser fácilmente encontradas y reusadas por otros. OpenML tiene alrededor de 19000 datasets disponibles para descargar y ofrece una API Web\footnote{\url{http://www.openml.org}} a través de la cual pueden ser enviados nuevos recursos y resultados. OpenML-Python \cite{feurer2019openmlpy} es una integración al ecosistema popular de Python ML\footnote{\url{https://github.blog/2019-01-24-the-state-of-the-octoverse-machine-learning/}}, que elimina la complejidad del acceso a la API Web proporcionando un fácil acceso en Python a todos los datos de OpenML y automatizando el intercambio de nuevos experimentos. Los datasets seleccionados fueron extraídos utilizando esta API.
 
 Para obtener un conjunto representativo de datasets se consideraron todos los que tenían más de 300  y menos de 500 000 instancias con más de 2 atributos y menos de 300 atributos, terminando en un total de 305 datasets. Estos datasets son muy diversos con respecto a su número de instancias, número de características, y número de clases. Características sobre su distribución pueden verse en la Figura \ref{fig:datasets}. El número de clases se muestra en escala logarítmica para un mejor análisis.
 
 \begin{figure}[H]
\centering
\includegraphics[scale=.73]{Figures/mtf-lineplot.pdf}
%\includegraphics[scale=.60]{Figures/mtf scatterplot.pdf}
\caption{Características de los datasets en cuanto al número de instancias, número de características y número de clases.}
\label{fig:datasets}
\end{figure}
 
 
 Para la evaluación de la propuesta realizada se separaron los 305 datasets en dos conjuntos: $D_{train}$ y $D_{test}$, donde el primero representa el $75\%$ del total de datasets y el segundo el $25\%$. $D_{train}$ fue utilizado en el entrenamiento de las estrategias seleccionadas y $D_{test}$ en la prueba de las mismas. %El conjunto de datasets de entrenamiento $D_{train}$ fue a su vez dividido en 2, usándose el segundo para la evaluación de los modelos implementados. De $D_{train}$ se usó el $75\%$ para el entrenamiento y el $25\%$ como validación, es decir, para obtener resultados parciales en cuanto al rendimiento del método implementado. 
 En los resultados mostrados en el resto de este capítulo, se usa todo el conjunto de datasets $D_{train}$ para el entrenamiento de los modelos obtenidos con las distintas estrategias implementadas y se exponen los resultados obtenidos en $D_{test}$.

 
 \section{Generación de Flujos}\label{sec:flujos}
 
% - Introducir la sección hablando de los tipos de algoritmos por los que está compuesto AutoGOAL (las bibliotecas que se usó, etc)
% - Hablar de cómo fue el proceso de entrenamiento de AutoGOAL para la extracción de solución, hablando de la configuración usada en AutoGOAL para la búsqueda de flujos.
% - Terminar hablando del tiempo total de entrenamiento, la cantidad total de flujos de algoritmos generados y el promedio de flujos generados por dataset.

Todos los flujos de algoritmos usados durante el entrenamiento y evaluación de la propuesta implementada fueron generadas usando AutoGOAL. Los flujos generados con AutoGOAL consisten en algoritmos presentes en varias bibliotecas de Python, entre las que se encuentran: Sklearn~\cite{scikit-learn}, Pytorch~\cite{paszke2019pytorch}, Keras~\cite{chollet2015keras}, NLTK~\cite{bird2009natural} y Gensim~\cite{khosrovian2008gensim}.

AutoGOAL se ejecutó en cada uno de los 305 datasets seleccionados y se almacenaron todas las arquitecturas generadas junto con los resultados obtenidos. Para cada uno de los datasets, AutoGOAL se configuró para que realizara la búsqueda de flujos durante 1 hora, teniendo un total de 5 minutos para la evaluación de un flujo de algoritmos. De esta forma, se excluyeron flujos muy complejos y se garantizó la evaluación de al menos 12 flujos por dataset. Esta generación de flujos duró un total de 305 horas y se generaron en promedio 640.28 flujos de algoritmos por dataset, y en total se obtuvieron 248 430  flujos. 

Estos flujos generados son utilizados como base de conocimiento para el método propuesto. Como se explica en la Sección~\ref{subsub:soluciones}, los flujos almacenados son utilizados para recomendar configuraciones iniciales en el proceso de optimización en dependencia de los datasets considerados similares al dataset que se quiere evaluar.

\section{Comparación de diferentes estrategias}\label{sec:comparacion}

%- Hacer un pequeño resumen de las configuraciones de las estrategias y variantes implementadas
%
%- Hacer un resumen del resto de la sección.

Los datasets y los flujos de algoritmos extraídos fueron utilizados con las diferentes estrategias de meta-learning implementadas:

\begin{description}
	\item[Estrategia Vecinos Cercanos Simple:] El método de vecinos cercanos, tal como está descrita en la Sección~\ref{subsub:nn}, fue probado utilizando la distancia L2 estándar. El ranking final generado para un nuevo dataset es de 15 flujos. En esta versión se utilizó la estrategia simple, en la que se seleccionan \texttt{n} datasets más cercanos y de ellos los \texttt{m} flujos de algoritmos que hayan obtenido un mejor rendimiento, luego el ranking es formado seleccionando a los 15 mejores flujos de algoritmos. Para la evaluación de esta estrategia se utilizó \texttt{m = 15} y \texttt{n = 15}.
	\item[Estrategia Vecinos Cercanos Ponderado:] El método de vecinos cercanos se vuelve a evaluar, pero utiliza la otra estrategia explicada en la Sección~\ref{subsub:nn}. Esta estrategia consiste en un mecanismo ponderado entre dos factores: la distancia entre el nuevo dataset y los datasets en el conjunto de entrenamiento, y el valor del resultado del rendimiento de los flujos de algoritmos en los datasets similares. Estos factores son divididos, y el valor obtenido es el usado para generar el nuevo ranking. Al igual que en la estrategia anterior, se utiliza la distancia L2 estándar y se genera un ranking final de 15 flujos para una tarea nueva.
	\item[Estrategia usando XGBRanker:] En esta estrategia, explicada en la sección~\ref{subsub:ranker}, se utilizó como meta-modelo XGBRanker de la biblioteca XGBoost para generar los rankings de una nueva tarea. Se usaron las siguientes configuraciones de hiperparámetros: 	
	\begin{itemize}
		\item \texttt{objective = `rank:pairwise'}: especifica la tarea de aprendizaje y el correspondiente objetivo de aprendizaje. En este caso, se eligió \textit{rank} porque es una tarea de \textit{ranking} y \textit{pairwise} porque utiliza el enfoque de ranking por pares (en inglés, \textit{pairwise ranking}).
		\item \texttt{n\_estimators = 150}: Es el número de los árboles con gradientes aumentados (\textit{gradient boosted trees}) utilizados en el algoritmo de ranking de XGBoost explicado en la Sección~\ref{subsub:ranker}.
		\item \texttt{tree\_method = `hist'}: especifica el algoritmo de construcción de árboles usados en XGBoost~\cite{xgboost}. XGBoost dispone de 4 métodos: \texttt{exact}, \texttt{approx}, \texttt{hist}, \texttt{gpu\_hist}. El algoritmo elegido fue \texttt{hist}, que es el algoritmo más rápido, ya que es optimizado mediante un algoritmo \textit{greedy}.
		\item \texttt{max\_depth = 10}: La profundidad máxima de los árboles utilizados como modelos básicos. El aumento de este valor hace al modelo más complejo y hace que sea más probable que ocurra sobreajuste.
		\item \texttt{learning\_rate = 0.1}: especifica la tasa de aprendizaje.
		\item \texttt{subsample = 0.95}: proporción de la submuestra tomada de las instancias de entrenamiento. El valor 0.95 indica que XGBoost muestrearía al azar el 95\% del conjunto de entrenamiento para prevenir el sobreajuste. Esto se realiza una vez en cada iteración del algoritmo.
%		\item \texttt{colsamplebytree = 0.9}: es la proporción de submuestra de las columnas al construir cada árbol. El submuestreo ocurre una vez para cada árbol construido.
		% 		\item \texttt{random_state = 43}: Número de semilla aleatoria usado en los procesos aleatorios.
 		\item \texttt{predictor = `cpu\_predictor'}: especifica el tipo de algoritmo de predicción a usar. Los algoritmos disponibles proporcionan los mismos resultados, pero permiten el uso de la GPU o la CPU. La configuración usada fue la CPU.
	\end{itemize}
\end{description}


%\subsection{Evaluación de la Precisión del Ranking}\label{subsec:ranking}
%
%%- Explicar que se midió la precisión del ranking obtenido en las diferentes estrategias con diferentes medidas
%%
%%- Explicar cada una de las métricas de evaluación usadas
%%
%%- Explicar cómo se realizó esta experimentación
%%
%%- Poner resultados.
%
%Uno de los indicadores de eficiencia de la propuesta desarrollada es la precisión del ranking obtenida en cada una de las propuestas llevadas a cabo. Para evaluar la eficacia de los modelos se compararon los rankings predichos para un determinado dataset con sus correspondientes etiquetas de ranking. Dado dos conjuntos de rankings de tamaño \texttt{k}: $T = [T_1, T_2, ..., T_k]$ y $P = [P_1, P_2, ..., P_k]$, los cuales son los valores de relevancia objetivos y predecidos respectivamente, las siguientes métricas de evaluación de ranking y funciones fueron usadas en los experimentos realizados:
%
%\begin{description}
%	\item[Coeficiente de Correlación de Rango de Spearman]o \textit{Spearman’s Rank Correlation Coefficient} (SRCC). SRCC evalúa que tan  bien puede ser descrita la relación entre el ranking verdadero y el predicho. Está definido como: $$\rho_{srccc} = 1 - \dfrac{6\sum^k_{i=1}(T_i - P_i)^2}{k(k^2-1)}$$
%	
%	\item[Coeficiente de Rango Ponderado]o \textit{Weighted Rank Correlation} (WRC). La métrica WRC le da más peso a los mejores candidatos. Ha sido usado en varios trabajos de meta-learning~\cite{sun2014MetaLearningAT, soares2004learning, costa2005weighted}. Está definido como: $$\rho_{wrc} = 1 - \dfrac{1-\sum^k_{i=1} (T_i - P_i)^2(2k - T_i - P_i + 2) }{k ^4+k^3-k^2-k}$$
%	
%	\item[Ganancia Acumulada Descontada Normalizada]o \textit{Normalized Discounted Cumulative Gain} (NDCG). Es una métrica de efectividad usada a menudo en motores de búsqueda usando una escala de relevancia calificada de elementos en una lista de resultados. Para entender la métrica NDGC es necesario entender primero las métricas de Ganancia Acumulada (\textit{Cumulative Gain}, CG) y Ganancia Acumulada Descontada (\textit{Discounted Cumulative Gain}, DCG).
%	
%	
%	CG es la suma de los valores de relevancia predecidas de todos los resultados en la lista resultante. Matemáticamente, $$CG = \sum^k_{i=1}P_i$$
%	
%	El problema de CG es que no tiene en cuenta el ranking del conjunto resultante cuando determina la utilidad del conjunto. En otras palabras, si se reordenase las puntuaciones de relevancias predichas no obtendríamos un mejor conocimiento de un conjunto resultante, ya que CG no cambiará.
%	
%	Para superar esto se introduce DCG. DCG penaliza los resultados con un mayor valor de relevancia que aparecen más abajo en la búsqueda, reduciendo la relevancia predicha en dependencia de la posición del resultado. Formalmente, $$DCG = \sum^k_{i=1} \dfrac{2^{P_i} - 1}{log_2(i + 1)}$$
%	
%	Un problema surge con DCG cuando se quiere comparar el resultado de diferentes rankings porque la lista de resultados puede variar en longitud. Por lo tanto, normalizando la ganancia acumulada en cada posición se llega a NDCG. Esto se realiza ordenando los valores objetivos por su relevancia relativa, produciendo el máximo valor posible en la posición \texttt{p}. Esta última medida se denomina Ganancia Acumulada Descontada Ideal (\textit{Ideal Discounted Cumulative Gain}, IDCG), que se calcula como: $$\sum^k_{i=1} \dfrac{2^{T_i} - 1}{log_2(i+1)}, $$ donde k es la cantidad de resultados relevantes.
%	Luego, NDCG se calcula como: $$NDCG = \frac{DCG_p}{IDCG}$$
%\end{description}
%
%La siguiente figura muestra los resultados obtenidos en cada una de las estrategias explicadas en la Sección~\ref{sec:comparacion}.
%
%[hablar un poco de los que se ve en las gráficas]
%
%%Por otro lado, también se obtiene un buen rendimiento al usar el sistema de recomendación de meta-learning para predecir flujos de algoritmos por sí solos. Es decir, sin añadir un extenso proceso de optimización posteriormente. Estas afirmaciones se pueden demostrar mediante los buenos resultados obtenidos para la predicción de rankings realizados.
%

\subsection{Resultados Experimentales}\label{subsec:resultados}

%- Explicar cómo se realizaron los experimentos.
%
%- Hablar de los diferentes aspectos evaluados (flujos inválidos, mejor resultado, iteraciones para el mejor resultado...).
%
%- Exponer los resultados.

%La eficacia del método implementado no se basa solo en qué tan bueno es para la creación de rankings  de flujos de algoritmos basados en experiencias pasadas. 

Para la evaluación de las estrategias de meta-learning implementadas es importante conocer los resultados obtenidos al incorporarse a un sistema de AutoML. Cómo el ranking de configuraciones elegidas son usadas para inicializar el proceso de optimización, y qué resultados este proceso puede obtener al usar el conocimiento previo adquirido mediante meta-learning. En esta sección se discuten las experimentaciones realizadas para evaluar estos aspectos.

Para la realización de los experimentos se ejecutó la búsqueda de algoritmos de AutoGOAL con y sin meta-learning, probando los métodos de meta-learning mencionados anteriormente: vecinos cercanos con la estrategia simple, vecinos cercanos utilizando mecanismos ponderados y utilizando un modelo de XGBoost, XGBRanker. Para estudiar su rendimiento bajo una estricta restricción de tiempo, y además, debido a limitaciones de los recursos computacionales utilizados, se limitó la búsqueda para cada ejecución a 30 minutos. Igualmente, el tiempo de ejecución de un solo modelo se limitó a la sexta parte de este tiempo (5 minutos). Las evaluaciones realizadas en esta sección muestran los resultados obtenidos en los datasets de prueba, utilizando los datasets de entrenamiento para entrenar los métodos implementados. % Cada estrategia se ejecutó 3 veces, y los resultados mostrados son el promedio de estas ejecuciones. 

Uno de los aspectos a analizar es la cantidad de flujos de algoritmos inválidos generados durante el proceso de optimización. Los flujos inválidos se encuentran en dos casos: cuando se excede el tiempo de espera predefinido por el investigador, o cuando ocurren errores de tiempo de ejecución impredecibles, como errores de falta de memoria provocados por una combinación inviable de hiperparámetros. Estas circunstancias a menudo son difíciles de predecir de antemano y no se pueden tener en cuenta en las gramáticas de AutoGOAL. Sin embargo, mediante la información adquirida con meta-learning se espera que la cantidad de flujos inválidos disminuya, ya que se utiliza conocimiento adquirido de problemas similares que usan flujos de algoritmos válidos.

En la Figura \ref{fig:failedpipelines}, se muestran los resultados obtenidos respecto a la proporción de flujos inválidos generados usando cada una de las estrategias desarrolladas, incluyendo la versión de AutoGOAL que no utiliza la inicialización de meta-learning. Como se puede observar, con las estrategias de meta-learning de Vecinos Cercanos Simple y XGBRanker se obtiene en promedio menor cantidad de flujos inválidos generados, mientras que en la estrategia de Vecinos Cercanos Ponderado se generan más flujos inválidos por dataset en promedio, por lo que no se puede determinar si la mejora obtenida con meta-learning es estadísticamente significativa. La generación de flujos inválidos es un factor que depende de muchos factores, y se cree que para demostrar la mejora en este aspecto es necesario una mayor experimentación.

\begin{figure}[H]
\centering
\includegraphics[scale=.8]{Figures/failed-pipelines.pdf}
\caption{Proporción de flujos de algoritmos inválidos generados en cada uno datasets usando las diferentes estrategias.}
\label{fig:failedpipelines}
\end{figure}

%Otro de los aspectos que puede ser interesante evaluar son las iteraciones necesarias para la obtención del mejor resultado. La Figura \ref{fig:maxidx} muestra los resultados obtenidos a través de las iteraciones en las diferentes versiones evaluadas. Se puede observar que las dos estrategias de vecinos cercanos obtuvieron el mejor resultado en menos iteraciones que AutoGOAL sin meta-learning. Sin embargo, la estrategia de XGBRanker no obtuvo el mismo resultado. % El resultado más sorprendente es que meta-learning produjo mejoras drásticas a partir de la primera configuración que seleccionó y que se prolongó hasta el final del experimento. La mejora fue más pronunciada al principio y, con el tiempo, AutoGOAL sin meta-learning también encontró buenas soluciones, lo que le permitió alcanzar los mismos resultados obtenidos en las versiones que usan meta-learning en algunos datasets (mejorando así su clasificación general).
%
%\begin{figure}[H]
%\centering
%\includegraphics[scale=.60]{Figures/max idx.pdf}
%\label{fig:maxidx}
%\caption{Cantidad de iteraciones necesarias para la obtención del mejor resultado promedio en cada uno datasets usando las diferentes estrategias.}
%\end{figure}

Otro de los aspectos interesantes para evaluar es el comportamiento de las soluciones obtenidas a través de las distintas iteraciones de AutoGOAL. La Figura~\ref{fig:performance} muestra la media y el intervalo de confianza del 95\% de los resultados de rendimiento obtenidos en las primeras 200 iteraciones en las diferentes estrategias evaluadas. Se puede observar como, a partir de determinado punto (cuando se terminan las iteraciones iniciales que generan resultados aleatorios) las soluciones que utilizan meta-learning alcanzaron mejoras drásticas a partir de la primera configuración que se seleccionó. La mejora fue más pronunciada al principio y, con el tiempo, AutoGOAL sin meta-learning también encontró buenas soluciones, lo que le permitió alcanzar los mismos resultados obtenidos en las versiones que usan meta-learning en algunos datasets (mejorando así su resultado final). 

%En la mayor parte del tiempo estos resultados se mantienen, por lo que las soluciones de meta-learning superan a las soluciones obtenidas mediante AutoGOAL sin el uso de meta-learning.

\begin{figure}[H]
\centering
\includegraphics[scale=.8]{Figures/performance.pdf}
\caption{Resultados de rendimiento obtenidos en los datasets de prueba en las primeras 200 iteraciones, se muestra la media y el intervalo de confianza del 95\%.}
\label{fig:performance}
\end{figure}

En la Figura \ref{fig:bestfn} se muestra el mejor resultado final obtenido en la búsqueda de los flujos de algoritmos en cada uno de los datasets de prueba para las diferentes estrategias. Como se puede apreciar, se obtienen mejores resultados finales en cada uno de las estrategias seguidas con respecto a AutoGOAL sin meta-learning. La diferencia entre la estrategia de vecinos cercanos simple no es tan pronunciada con respecto a  la versión que no usa meta-learning, y esto se cree que se debe a que, a pesar de que mediante esta estrategia se obtienen los valores óptimos más rápidos, AutoGOAL es capaz de alcanzar a esta versión, y encontrar buenas soluciones. El método de vecinos cercanos ponderado y el que usa XGBRanker, un algoritmo de ranking de XGBoost, sí obtiene resultados un poco más significativos, siendo este último el que mejor resultado final obtiene. Por lo tanto, se puede concluir que la agregación de conocimiento de experiencias pasadas mediante el método de meta-learning implementado mejoran la búsqueda final de flujos de algoritmos, obteniendo flujos que tienen mejores resultados en la mayoría de los datasets en el mismo intervalo de tiempo.

\begin{figure}[H]
\centering
\includegraphics[scale=.8]{Figures/best-fn.pdf}
\caption{Mejor resultado de rendimiento obtenido en la búsqueda de los flujos de algoritmos en cada uno de los datasets de prueba para las diferentes estrategias.}
\label{fig:bestfn}
\end{figure}

La Figura~\ref{fig:zscore} muestran los resultados obtenidos estandarizados. Los valores atípicos fueron omitidos para un mejor análisis de la gráfica. Para su normalización, fue usada la Puntuación Z o \textit{Z-Score}, que se utiliza en estadística para comparar datos procedentes de diferentes muestras o poblaciones. Se define como el número de desviaciones típicas que un valor dado toma con respecto a la media de su muestra o población. Sea $x$ el mejor resultado alcanzado en un dataset, $\mu$ y $\sigma$ la media y la desviación estándar de los resultados obtenidos en ese dataset, el valor z de este dataset es: $$z = \dfrac{x - \mu}{\sigma}$$

Los resultados de la Figura~\ref{fig:zscore} muestran una pequeña mejora en la estrategia de Vecinos Cercanos Simple y XGBRanker con respecto a AutoGOAL, mientras que no se puede apreciar mucha diferencia con respecto a los resultados obtenidos utilizando el método de Vecinos Cercanos Ponderados. Sin embargo, es necesario tener en cuenta que esta métrica no fue la usada en el entrenamiento de los modelos de meta-learning. Las estrategias que usan directamente el resultado de rendimiento, y no este valor normalizado, como la de Vecinos Cercanos Ponderado y XGBRanker, pueden estar afectadas en la comparación con esta métrica. A pesar de esto, se puede concluir que los resultados alcanzados por AutoGOAL usando las estrategias de meta-learning superan aquellos obtenidos por AutoGOAL sin meta-learning.

\begin{figure}[H]
	\centering
	\includegraphics[scale=.8]{Figures/z-score.pdf}
	\caption{Puntuación Z obtenida en la búsqueda de los flujos de algoritmos en cada uno de los datasets de prueba para las diferentes estrategias.}
	\label{fig:zscore}
\end{figure}

Los resultados experimentales fueron implementados utilizando el mismo código en todos los experimentos. Para cada una de las variantes analizadas, los experimentos se ejecutan bajo la misma restricción de tiempo (30 minutos de ejecución total y 5 minutos de ejecución para un solo modelo), por lo tanto, cada una de las estrategias probadas tiene el mismo tiempo para la búsqueda de los flujos de algoritmos. Las experimentaciones realizadas muestran que el conocimiento adicional obtenido al principio mediante la técnica de meta-learning desarrollada cuando es agregado al proceso de optimización, obtiene resultados satisfactorios. En cuanto a la cantidad de flujos inválidos generados y la velocidad con la que se encuentra el mejor resultado de rendimiento, en promedio, los modelos que usan la técnica de meta-learning obtienen mejores resultados con respecto al modelo donde no se aplica.


% Falta hablar más de los resultados, sobre todo que quede claro la mejora o la diferencia...

\section{Discusión}\label{sec:discusion}

%- Hablar de la evidencia de que el sistema funciona 
%
%- Limitaciones:
%
%	+ Tener en cuenta métricas multi-objetivos
%	
%	+ Adaptarlo para usar regresión
%	
%	+ Hablar de las limitaciones de los datasets, de mayor variedad y de una mayor diversificación en sus características
%	
%	+ Hablar de los meta-features, del estudio la posible selección de estas


El método propuesto tiene también algunas limitaciones, las cuales se pueden mejorar en trabajos futuros. Con respecto a la métrica usada para determinar el rendimiento de cada flujo, se usa la métrica por defecto de AutoGOAL, \textit{accuracy}. Sin embargo, en escenarios prácticos, puede ser necesario equilibrar diferentes métricas de rendimiento, incluido también el uso del tiempo y la memoria, y cualidades más subjetivas como la interpretabilidad de los modelos o su capacidad para lidiar con datos sesgados. El enfoque de optimizar una métrica principal sujeta a restricciones de tiempo y memoria usada en AutoGOAL, y, por lo tanto, también en la propuesta diseñada, es insuficiente en un escenario en el que el usuario final tiene que decidir sobre cuestiones prácticas como el despliegue de estos flujos en un sistema de producción. % A modo de ejemplo, TPOT~\cite{olson2019tpot} considera este problema desde el enfoque multi-objetivo mediante la optimización conjunta de la precisión y la complejidad del modelo (en términos de longitud de los flujos). También se han implementado eficientemente medidas multi-criterio en sistemas de meta-learning~\cite{bradzil2003ranking}, donde se crea el ranking de algoritmos basado en \textit{accuracy} y tiempo, mediante la métrica denominada \textit{Adjusted Ratio of Ratios} (ARR)~\cite{soares2000measures}

La experimentación llevada a cabo se realizó solo con datasets de clasificación, por lo que solo se estudiaron los resultados obtenidos en esa tarea. Para demostrar la utilidad de la estrategia de meta-learning implementada en una gran gama de tareas es necesario la experimentación en diferentes tipos de tareas de aprendizaje, por ejemplo, de regresión.  Sin embargo, el modelo propuesto es adaptable a cualquier tipo de problema de aprendizaje automático siempre y cuando se tenga una medida para determinar la eficiencia de un flujo de algoritmos en un dataset. Para la realización de varios tipos de tareas, se propone la creación de un modelo diferente para cada tipo de problemas. Es decir, se crearía un modelo para las tareas de clasificación y otro para regresión. De esta manera, para el análisis de un dataset es necesario conocer el tipo de tarea que se quiere resolver, y usar el modelo adecuado para ella.

Meta-learning es muy dependiente de la calidad de los meta-ejemplos que usa para su entrenamiento~\cite{gomes2012combining}. Usualmente, es difícil obtener buenos resultados, ya que las meta-características son frecuentemente bastante ruidosas, porque el cálculo de estas está propenso a errores. Además, el número de problemas disponibles para la generación de meta-ejemplos generalmente es limitada. En este trabajo, se obtienen buenos resultados debido al uso de meta-características que han sido ampliamente utilizadas en la literatura y a la gran variedad de datasets extraídos. Sin embargo, se considera que un análisis más detallado de la distribución de los datasets para excluir aquellos que tienen características atípicas, y la inclusión de datasets de mayor tamaño puede proporcionar mejores resultados. De igual manera, un estudio de las meta-características para la eliminación de aquellas que proporcionen información poco útil debe proporcionar mejoras en el método propuesto. Además, el uso de la medida normalizada (\textit{Z-Score}) en el entrenamiento de las estrategias implementadas, puede proporcionar mejoras para la selección justa de flujos de algoritmos en distintos datasets.





