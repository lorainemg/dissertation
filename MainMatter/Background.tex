\chapter{Estado del arte }\label{chapter:review}

 \begin{itemize}
 	\item Realizar una introducción sobre el contenido 
 	\item Hablar un poco de la estructura del capítulo.
 	 \item En la sección de Meta-Learning se define el problema de meta-learning, se dan definiciones del mismo y se realiza un estudio de meta-learning aplicado al problema de la selección de modelos separados por el tipo de solución que se da para el mismo ~\ref{sec:metalearning}
 	\item La sección de AutoML se define el problema de AutoML, se da una (o varias) definiciones del concepto de AutoML y se realiza un estudio del mismo desde los diferentes tipos de métodos de optimización usados para la selección de modelos, destacando el uso de meta-learning. ~\ref{sec:automl}
 \end{itemize}

\section{Meta-Learning}\label{sec:metalearning}

\begin{itemize}
	\item[$\checkmark$] Introducir esta sección hablando un poco del concepto de meta-learning y un poco de su historia (referirse a los papers: \textit{Metalearning: A survey of trends and technologies} y \textit{A Comprehensive Overview and Survey of Recent Advances in Meta-Learning})
\end{itemize}

El término meta-learning ocurrió por primera vez en el área de psicología educacional. Uno de los investigadores más citados en este campo, John Biggs, describió meta-learning como \textit{ser consciente y tomar el control del conocimiento de uno}. Por lo tanto, meta-learning es visto como un entendimiento y adaptación del aprendizaje en sí en un nivel más alto que simplemente adquirir conocimiento de una materia. De una forma, una persona consciente y capaz de meta-learning es capaz de evaluar su enfoque de aprendizaje de acuerdo a los requerimientos de una tarea en específico.

Meta-learning usada en el contexto de aprendizaje de máquina tiene muchas similitudes a esta descripción. El conocimiento de una materia se traduce en \textit{base-learning}, donde la experiencia es acumulada para una tarea en específico. Meta-learning empieza en un nivel mayor y se encarga de acumular experiencia sobre varias aplicaciones de un sistema de aprendizaje de acuerdo a..[poner link]

En los últimos 20 años, la investigación de aprendizaje de máquinas enfrentó un creciente número de algoritmos disponibles, incluyendo multitudes de parametrizaciones, enfoques de pre-procesamiento y post-procesamiento, así como una gran variedad de aplicaciones debido al creciente poder de computación y gran disponibilidad de conjuntos de datos. Promoviendo un mejor entendimiento del aprendizaje de máquinas en sí, meta-learning puede ser de un ayuda invaluable, evitando procedimientos extensivos de prueba y error para la selección de algoritmos. Además, puede permitir entender mejor que hace a un determinado algoritmo desempeñarse bien en un determinado problema.

\subsection{Definición}\label{subsec:mtl-definition}

\begin{itemize}
	\item[$\checkmark$] Hablar de distintas definiciones formales del término de meta-learning (referirse a:  \textit{Metalearning: A survey of trends and technologies})
\end{itemize}

La primera definición de \textit{meta-learning} en el campo de aprendizaje de máquinas fue dado por J\"ugen Schmidhuber en 1987, el cual lo considera la interacción entre agente y el ambiente impulsando la superación personal en el agente. Un número de definiciones ha sido dada desde entonces, la siguiente lista fue obtenida de:

\begin{enumerate}
	\item Meta-learning estudia como los sistemas de aprendizaje puede incrementar en eficiencia a través de la experiencia; el objetivo es entender cómo el aprendizaje en sí puede hacerse flexible de acuerdo al dominio o la tarea sujeto de estudio.
	\item El principal objetivo de \emph{meta-learning} es entender la interacción entre el mecanismo de aprendizaje y los contextos concretos en los cuales ese mecanismo es aplicable.
	\item \emph{Meta-learning} es el estudio de los métodos principales que explotan el meta-conocimiento para obtener modelos eficientes y soluciones adaptando los procesos de aprendizaje de máquinas y minería de datos.
	\item \emph{Meta-learning} monitorea el proceso de aprendizaje automático en sí, en el contexto de aprender los problemas que encuentra, e intenta adaptar su comportamiento para desempeñarse mejor.
\end{enumerate}

En la mayoría de las definiciones son conceptos claves sistemas de aprendizaje que se adaptan y mejoran con la experiencia, excepto en la definición 2. Esta enfatiza en una mejor comprensión de la interacción entre dominios y mecanismos de aprendizaje, lo cual no necesariamente implica el objetivo de un sistema de aprendizaje mejorado, sino la búsqueda de un mejor entendimiento de cuales tareas tienen éxito o fallan.

\emph{Meta-learning} es mejor entendido comúnmente como \textit{aprendiendo a aprender}, lo cual se refiere al proceso de mejorar un algoritmo de aprendizaje a través de múltiples episodios de aprendizaje. En contraste, el aprendizaje de máquinas convencional mejora las predicciones del modelo sobre múltiples instancias de datos. Durante el \textit{base-learning} o aprendizaje base, un algoritmo de aprendizaje interior (o inferior/base) resuelve una \textit{tarea} como clasificación de imágenes, definida por una dataset y un objetivo. Durante \emph{meta-learning}, un algoritmo externo (o superior/meta) actualiza el algoritmo interior de tal forma que el modelo que aprende mejore el objetivo exterior. Los episodios de aprendizaje de la tarea base pueden ser vistos como una forma de proveer las instancias necesitadas por el algoritmo externo para aprender el algoritmo de aprendizaje base. 

Meta-learning difiere de \textit{base-learning} en el alcance del nivel de adaptación. Mientras que el aprendizaje en un nivel base está enfocado en acumular experiencia en una tarea específica, el aprendizaje en meta-learning tiene el objetivo de acumular experiencia en el rendimiento de múltiples aplicaciones de un sistema de aprendizaje.  

De esta forma, muchos algoritmos convencionales tales como la búsqueda aleatoria de hiperparámetros mediante validación cruzada podrían caer en la definición de \emph{meta-learning}. La característica destacada del \emph{meta-learning} contemporáneo es un meta-objetivo explícitamente definido, y una optimización de extremo a extremo del algoritmo interior con respecto a este objetivo.

\subsection{Campos relacionados}\label{subsec:mtl_related_fields}

\begin{itemize}
	\item[$\checkmark$] Mencionar y resumir algunas áreas relacionadas con meta-learning, para incluir AutoML (referirse a: \textit{Meta-Learning in Neural Networks: A Survey})
	\item No estoy segura si esta sección es necesaria...
\end{itemize}

Aquí posicionamos meta-learning contra áreas relacionadas cuya relación con meta-learning es a menudo una fuente de confusión:

\begin{description}
	\item[Transfer Learning (TL):] TL usa experiencia pasada de una tarea fuente para mejorar el aprendizaje (la velocidad, la eficiencia de los datos, la precisión) de una tarea destino. TL se refiere a esta área de problemas como familia de soluciones, y para su solución hace uso de la transferencia de parámetros, más el ajuste opcional de los mismos. En contraste, meta-learning se refiere al paradigma que puede ser usado para mejorar TL, así como otros problemas. En TL el modelo final es extraído por aprendizaje simple en la tarea fuente sin el uso de un meta-objetivo. En \emph{meta-learning}, el modelo final estaría definido por la optimización externa que evalúa el beneficio del modelo cuando aprende una nueva tarea. De una forma más general, \emph{meta-learning} trata con un rango mucho más grande de meta-representaciones.
	\item[Aprendizaje multi-tarea (MTL):] intenta aprender en conjunto algunas tareas relacionadas, para beneficiarse de la regularización debido al intercambio de parámetros y la diversidad de la representación compartida resultante. Como TL, MTL convencional es una optimización de un solo nivel sin un meta-objetivo. Además, el propósito de MTL es el de aprender de una cantidad fija de tareas conocidas, mientras que el punto de meta-learning es a menudo aprender de taras futuras no vistas. 
	\item[Optimización de Hiperparámetros (HO):] está dentro de las consideraciones de meta-learning, en el sentido de que hiperparámetros como la tasa de aprendizaje o la fuerza de regularización describen “como aprender”. Aquí se suelen incluir tareas de HO que definen un meta-objetivo que es entrenado de extremo a extremo con redes neuronales, tales como el aprendizaje de hiperparámetros basados en gradientes y la búsqueda de arquitectura neuronal. Pero excluyen otros enfoques como búsqueda random y optimización bayesiana, las cuales raramente están consideradas como meta-learning.
	\item[AutoML:] AutoML es más bien una sombrilla amplia para los enfoques con el objetivo de automatizar partes del proceso de aprendizaje de máquinas que son típicamente manuales, tales como la preparación de los datos, la selección de algoritmos, ajuste de hiperparámetros, y búsqueda de arquitecturas. AutoML a menudo hace uso de numerosas heurísticas afuera del alcance de meta-learning tal y como está definido aquí, y se centra en tareas como limpieza de datos, que son menos centrales a meta-learning. Sin embargo, AutoML a veces hace uso de optimizaciones de extremo a extremo de un meta-objetivo, así que meta-learning puede ser visto como una especialización de AutoML.
\end{description}

\subsection{Estructura de un Sistema de Meta-Learning}

Un sistema de meta-learning está compuesto esencialmente por dos partes. Una parte tiene la tarea de adquirir meta-conocimiento de sistemas de aprendizaje automático. La otra parte tiene el objetivo de aplicar este meta-conocimiento a nuevos problemas con el objetivo de identificar un algoritmo o técnica de aprendizaje óptimo.

\subsubsection{Adquisición de meta-conocimiento} 

Hay dos formas naturales en las cuales el meta-conocimiento puede ser adquirido. Una posibilidad es depender de conocimiento experto y otra posibilidad es usar un procedimiento automático. 

La obtención de meta-conocimiento a través del conocimiento experto se realiza en la forma de reglas que coinciden con las características de dominio (dataset) y con algoritmos de aprendizaje de máquinas. Estas reglas pueden ser hechas a mano, teniendo en cuenta resultados teóricos, el conocimiento humano y evidencia empírica. Sin embargo, este método tiene serias desventajas: el conjunto de reglas resultantes probablemente esté incompleto y el mantenimiento a tiempo y preciso del conjunto de reglas a medida que nuevos algoritmos se vuelven disponibles es problemático. Como resultado, la mayoría de la investigación se ha enfocado en métodos automáticos.
	
Para la automatización de la adquisición de conocimiento es necesario un conjunto de problemas (datasets) y un conjunto de algoritmos de aprendizaje de máquinas que queremos considerar. Entonces se necesita definir un método experimental que determine con cuáles alternativas deberíamos experimentar y en qué orden. Suponguemos que tenemos un dataset (caracterizado por ciertas meta-features) y ciertos algoritmos de aprendizaje automático. Para cada uno de estos algoritmos se producen resultados de rendimiento a través de un método de evaluación (por ejemplo, validación cruzada). Los resultados, junto con la caracterización, represetan un pedazo de información que es guardado en una base de meta-conocimiento. El proceso es entonces repetido para otras combinaciones de datasets y algoritmos.

\subsubsection{Aplicación de meta-conocimiento}

En un sistema de meta-learning la aplicación de meta-conocimiento puede ser usado para ayudar a seleccionar o adaptar algoritmos de aprendizaje de máquinas. Por ejemplo, podemos considerar nuestro problema de la selección de algoritmos de aprendizaje automático dado un determinado conjunto. El problema puede ser visto como un problema de búsqueda, donde el espacio de búsqueda incluye los algoritmos de aprendizaje de máquinas individuales y el objetivo es identificar el mejor algoritmo. Este proceso puede ser dividido en dos fases separadas. 

En la primera fase el objetivo es identificar un subconjunto adecuado de algoritmos de aprendizaje automático basados en un dataset de entrada. El método de selección usado en ese proceso se basa en el meta-conocimiento. Por lo general, el resultado de esta fase es representado en la forma de un subconjunto rankeado de algortimos de aprendizaje de máquinas.

La segunda fase es usada para buscar a través del espacio reducido. Cada opción es evaluada usando una métrica de rendimiento determinado (por ejemplo, \textit{accuracy}). Usualmente, la validación cruzada es usada para identificar el mejor algoritmo de aprendizaje. Es necesario notar que el meta-conocimiento no elimina completamente la necesidad del proceso de búsqueda, sino que proporciona una búsqueda más efectiva. La efectividad de la búsqueda depende de la calidad del meta-conocimiento.


\subsection{Aplicaciones de Meta-Learning}\label{subsec:mtl_aplications}

Meta-learning puede ser empleada en una variedad de configuraciones, con cierto desacuerdo en la literatura sobre lo que constituye exactamente un problema de meta-learning. Meta-learning es extremandamente útil en los casos donde el modelo interno es requerido y hay poca cantidad de datos, ya que el modelo interno contiene muchos parámetros que no pueden ser estimados precisamente con pocos datos. Algunas de las aplicaciones comunes son en la investigación robótica, donde se espera que los robots tengan un mayor nivel de autonomía en IA general, en el descubrimiento de drogas para manejar los datos de altas dimensiones con un tamaño de muestra pequeño y en la traducción de lenguajes raramente usados. Además, meta-learning es ampliamente usado en el problema de selección de algoritmos, sobre esta aplicación se profundiza en el sección \ref{sec:metalearning-automl}.

Meta-learning constituye una solución factible para los problemas donde una definición específica de \textit{tarea} y \textit{label} puede ser claramente especificada. Un sistema de meta-learning es flexible y puede ser integrado convenientemente con la mayoría de los algoritmos de aprendizaje de máquinas para proporcionar soluciones factibles. Para las tareas que son computacionalmente costosas, meta-learning presenta la opción de agregación o adaptación de los resultados anteriores para salvar recursos computacionales.
 
\section{AutoML}\label{sec:automl}

\begin{itemize}
	\item Introducir el tema de AutoML y su próposito
\end{itemize}

\subsection{Definición}\label{subsec:automl_definition}


\begin{itemize}
	\item[$\checkmark$] Definición de AutoML, quizá desde distintas perspectivas usando diferentes definiciones
	\item[$\checkmark$] Explicar además varias de las subtareas de AutoML \begin{itemize}
		\item[$\checkmark$] \textit{Automated feature enginineering}
		\item[$\checkmark$] \textit{Automated model selection}
		\item[$\checkmark$] \textit{Neural architecture search}
	\end{itemize}
\end{itemize}

\textit{Automated Machine Learning} (AutoML) o Aprendizaje de Máquinas Automático es el campo que se enfoca en los métodos que tienen el objetivo de automatizar diferentes etapas del proceso de aprendizaje de máquinas. Como su nombre indica, AutoML es la intersección de dos campos: automatización y aprendizaje de máquinas. Las soluciones de AutoML están recibiendo incrementalmente más atención tanto por la comunidad de ML como por los usuarios por (1) las grandes cantidades de datos disponibles en todas partes y (2) la falta de expertos de ML que puedan supervisar/asesorar el desarrollo de sistemas basados en ML.

La diferencia entre el aprendizaje de máquinas clásico y AutoML es que en el primero, los humanos están grandemente involucrados en la configuración de las herramientas de aprendizaje realizando ingeniería de características, selección de modelos y evaluación de modelos. Como resultado, los humanos realizan la mayor parte del trabajo en las prácticas del aprendizaje de máquinas. Sin embargo, en AutoML, todo esto puede ser hecho con programas de forma automática.

La comunidad de AutoML se ha centrado en resolver varias partes de un flujo de trabajo de aprendizaje automático estándar. Algunos ejemplos de estas partes o subtareas que son aplicadads en AutoML son:

\begin{description}
	\item[\textit{Automated Data Preparation} o Preparación de Datos Automático:] el flujo de trabajo de la preparación de datos está compuesto por 3 componentes: colección de datos, limpieza de datos e incremento de datos. La colección de datos es un paso necesario para construir un nuevo dataset o extender el dataset existente. El proceso de limpieza de datos es usado para filtrar los datos con ruido para que el entrenamiento del modelo no sea comprometido. El incremento de datos tiene un rol importante en mejorar la robustez del modelo y mejorando el rendimiento del modelo. Esta paso es uno de los que más dificilmente son automatizados
	
	\item[\textit{Automated Feature Engineering} o Ingeniería de Características Automático:] el objetivo de ingeniería de características es construir más características para mejorar el rendimiento de aprendizaje cuando las características no son lo suficientemente informativas. Para esto se construye un modelo que aprende de las características de entrada para construir nuevas características con las cuales el algoritmo de aprendizaje automático obtiene un mejor rendimiento. Con la automatización de esta tarea se elimina parte de la asistencia humana para construir automáticamente nuevas características basadas en la interacción entre las características de entrada.
	
%	\item[\textit{Automated Model Selection} o Selección Automática de Modelos:] el objetivo de la selección de modelos es encontrar un algoritmo de aprendizaje de máquinas adecuado para un dataset determinado. Hay muchos aspectos por los cuales un científico de datos se preocupa en la selección de algoritmos, tales como la complejidad computacional, diferencias en el tiempo de entrenamiento y si el algoritmo permite una entrada no lineal, y es útil considerar estos aspectos en la automatización. Para realizar esta tarea un sistema AutoML realiza una búsqueda sobre un conjunto de algoritmos disponibles, con el objetivo de determinar cuáles pertenecen al flujo óptimo dado un problema determinado.

	item[\textit{Model Evaluation} o Evaluación del Modelo:] Una vez que el modelo ha sido generado, su rendimiento debe ser evaluado. Un método intuitivo consiste en entrenar el modelo hasta que converja y entonces evaluar su rendimiento. Sin embargo, este método requiere un tiempo y unos recursos computacionales extensivos. Por lo tanto, existen varios métodos para acelerar el proceso de evaluación de modelos. Algunos ejemplos de estos métodos son \textit{early stopping} (explicar). El uso de \textit{Low fidelity} y de \textit{surrogate models} o modelos sustitutos son otras técnicas de evaluación que serán explicadas más adelante (poner link)
	
	\item [\textit{Neural Architecture Search} (NAS) o Búsqueda de Arquitecturas Neuronales:] el objetivo de NAS es encontrar una arquitectura de redes neuronales profundas con buen rendimiento en un dataset determinado. NAS ha sido usado para diseñar redes que están a la par o tienen mejores resultados que arquitecturas diseñados a mano [poner ref de wiki]. Aunque es una práctica común optimizar la arquitectura y las configuraciones de los hiperparámetros en secuencia, existe evidencia reciente de que deberían ser optimizadas en conjunto [hpo fundations].
\end{description}

\subsection{Definición del problema}\label{subsec:automl_problem_definition}

\begin{itemize}
	\item[$\checkmark$] Definir el problema de automl en términos de CASH
	\item[$\checkmark$] Definir el problema de optimización de hiperparámetros (HPO) 
	\item[$\checkmark$] Definir el problema de \textit{Combined Algorithm Selection and Hyperparameter tuning} (CASH)
%	\item Definir el workflow de un sistema de AutoML \begin{itemize}
%		\item Su entrada es un conjunto de datos, una métrica de optimización (poner ejemplos de métricas de optimización) y (generalmente) restricciones de tiempo.
%		\item Su salida es un modelo (pipeline?) de machine learning con sus hiperpárametros.
%		\item El sistema de AutoML está fundamentalmente definida por la técnica de optimización usada y la definición del espacio de búsqueda.  
%	\end{itemize}
\end{itemize}

Dos problemas importantes en AutoML son (1) que ningún algoritmo de ML obtiene los mejores resultados en todos los datasets (también conocido como \textit{No Free Lunch Problem}, poner link) y (2) que algunos métodos de aprendizaje de máquinas dependen crucialmente de la optimización de hiperparámetros. Para la resolución de estos problemas AutoML se apoya de dos áreas o subtareas que consituyen su base: la selección de modelos (\textit{Model Selection}, MS) y la optimización de hiperparámetros (\textit{Hyperparameter Optimization}, HPO). La combinación de estas áreas se refiere al problema de AutoML como un problema de selección combinada de modelos y optimización de hiperparámetros (\textit{Combined Algorithm Selection and Hyperparameter Optimization}, CASH), que se describe en la presente sección.

\subsubsection{Selección de Modelos}

El objetivo de la selección de modelos es encontrar un algoritmo de aprendizaje de máquinas adecuado para un dataset determinado. Hay muchos aspectos por los cuales un científico de datos se preocupa en la selección de algoritmos, tales como la complejidad computacional, diferencias en el tiempo de entrenamiento y si el algoritmo permite una entrada no lineal, y es útil considerar estos aspectos en la automatización. Para realizar esta tarea un sistema AutoML realiza una búsqueda sobre un conjunto de algoritmos disponibles, con el objetivo de determinar cuáles pertenecen al flujo óptimo dado un problema determinado.

\begin{definition}
	Dado un conjunto A de algortimos de aprendizaje de máquinas y una cantidad limitada de datos $D = \{(x_i, y_1),..., (x_n, y_n)\}$, el objetivo de la selección de modelos es determinar el $a^* \in A$ con un rendimiento óptimo general. Para una evaluación general se divide el conjuto de datos $D$ en $D_{train}$ y $D_{valid}$ disjuntos para entrenamiento y validación respectivamente, se aprenden funciones $f_i$ al aplicar $a^*$ a $D_{train}$, y se evalúa el rendimiento de las funciones en $D_{valid}$. Más formalmente, el problema de selección de modelos consiste en encontrar

	\begin{equation}
		a^* = argmin_{a \in A}\dfrac{1}{k}\sum_{i=1}^K \textbf{L}(a, D_{train}, D_{valid})
	\end{equation}
	
	donde $L(a, D_{train}, D_{valid})$ es la pérdida de $a$ al ser evaluado en $D_{valid}$ y entrenado con $D_{train}$.
	
%	 El protocolo de validación puede tomar distintas formas; una de las más populares opciones es el cálculo del error de validación cruzada con una función de pérdida definida por el usuario.

\end{definition}

\subsubsection{Optimazión de hiperparámetros}

Los algoritmos de aprendizaje de máquinas son altamente configurables por sus hiperparámetros (HP). A menudo influencian substancialmente el comportamineto y la velocidad del algoritmo y tienen que ser seleccionados con cuidado con el objetivo de alcanzar un rendimiento óptimo. Tanto para expertos como para no expertos, ajustar los hiperparámetros para optimizar el rendimiento del modelo de extremo a extremo puede ser una tarea difícil y tediosa, a menudo es bastante constosa y propensa a errores, especialmente cuando son seleccionados en un proceso de prueba y error. Los algoritmos de optimización de hiperparámetros (HPO) identifican automáticamente una configuración de hiperparámetros (HPC) buena para un algoritmo de ML, reduciendo así el esfuerzo humano. Por lo tanto, una de las tareas fundamentales de AutoML es ajustarlos para optimizar el rendimiento de los algoritmos de ML. 

\begin{definition}
	Sean A el conjunto de algoritmos de aprendizaje de máquinas y $a \in A$ un algoritmo con $N$ parámetros. Denotamos el dominio del n-ésimo parámetro como $\Lambda_n$ y el espacio de configuraciones como $\Lambda = \Lambda_1 \times \Lambda_2 \times ... \times \Lambda_x$. Un vector de hiperparámetros es denotado por $\lambda \in \Lambda$, y $a$, con sus hiperparámetros instanciados en $\lambda$, por $a_\lambda$.
\end{definition}


Existe gran variedad de dominios en los hiperparámetros, pueden tener valores continuos, discretos, booleanos o categóricos. [poner ejemplos]. 

Este espacio ya mixto puede desafortunadamente contener \textit{hiperparámetros dependientes}, lo que conlleva a un espacio de búsqueda jerárquico: 

\begin{definition}
	Un HP $\lambda_i$ se considera \textit{condicional} con $\lambda_j$ si $\lambda_i$ solo es activo cuando $\lambda_j$ es un elemento de un subconjunto dado de $\Lambda_j$ y está inactivo, es decir, no afecta al algoritmo resultante [poner link].  
\end{definition} 

Poner ejemplos

A causa de esta variedad en los espacios de hiperparámetros, los algortimos de optimización convecionales, o no se aplican directamente, u operan sin aprovechar la estructura valiosa en el espacio de configuración.

Entonces según la definición de [sklearn], dado un conjunto de datos $D$, el problema de HPO general está definido como:

\begin{equation}
	\lambda^* = argmin_{\lambda \in \Lambda}         \textbf{E}_{(D_{train}, D_{valid})}\textbf{V}(L, a_\lambda, D_{train}, D_{valid})
\end{equation},

donde $\textbf{V}(L, a_\lambda, D_{train}, D_{valid})$ mide la pérdida de un modelo generado por $a_\lambda$ en el conjunto de entrenamiento $D_{train}$ y evaluado sobre el conjunto $D_{valid}$. Al igual que en la selección de modelos, hay distintos métodos de validación que pueden ser usados para comprobar los resultados, uno de los más populares es la validación cruzada.

\subsubsection{AutoML como problema de CASH} 

El problema de resolver MS y HPO de forma independiente es que ningún algoritmo rinde mejor en todos los conjuntos de datos y existen algoritmos que dependen decisivamente de sus parámetros. Afortunadamente, estos dos problemas pueden ser abordados como un problema de optimización simple, estructurado y conjunto:

\begin{definition}
	Sea $A = \{a^1, a^2, ..., a^k\}$ un conjunto de algoritmos con espacios de configuraciones asociados $\Lambda_1, ... \Lambda_k$. Sea D un conjunto de datos que se divide en $\{D_{train}^{(1)}, ..., D_{train}^{(k)}\}$ y $\{D_{valid}^{(1)}, ..., D_{valid}^{(k)}\}$ tal que $D^{(i)}_{train} = D_{train}\backslash D_{valid}^{(i)}$, para $i=1,...,K$. Finalmente sea $L(a_\lambda^{(j)}, D_{train}^{(i)}, D_{valid}^{(i)})$ denotan la pérdida del algoritmo $a^{(j)}$ en el dataset $D_{valid}$ cuando es entrenado en el dataset $D_{train}$. Luego, el problema de la \textsl{Selección Combinada de Modelos y Optimización de Hiperparámetros (CASH)} consiste en encontrar el algoritmo y la configuración de hiperparámetros que minimice la pérdida:
	
	\begin{equation}\label{cash}
	 a^*_{\lambda^*} \in argmin_{a^{(j) \in A, \lambda \in \Lambda^{(j)}}} \dfrac{1}{k} \sum_{i=1}^{k} L\left(a_\lambda^{(j)}, D_{train}^{(i)}, D_{valid}^{(i)}\right)
	\end{equation}
\end{definition}   

%En la práctica, una de las restricciones de las técnicas de optimización de CASH es el \textit{presupuesto de tiempo}. En particular, el objetivo del algoritmo de optimización es seleccionar y ajustar un algoritmo de aprendizaje de máquinas que puede lograr rendimiento casi óptimo en términos de métricas de evaluaciones definidas por el usuario (por ejemplo, accuracy, sensitivity, specificity, F1-score) dentro del \textit{presupuesto de tiempo} definido por el usuario.

En la práctica, el problema de CASH es reformulado como un solo problema de optimización de hiperparámetros jerárquico con el espacio de parámetros $\Lambda^{(1)}\cup ... \cup \Lambda^{(k)} \cup\{\lambda_r\}$, donde $\lambda_r$ es un nuevo hiperparámetro que selecciona entre los algoritmos $Aˆ{(1)}, ..., Aˆ{(k)}$. Todos los parámetros de cada subespacio $\Lambda^{(i)}$ se vuelven condicionales de $\lambda_r$ al ser inicializado como uno de los algoritmos $A_i$.
 
\subsection{Estrategias de Búsqueda}\label{subsec:automl_methods}

\begin{itemize}
	\item[$\checkmark$] Introducir esta sección
	\item A lo largo de esta sección, mi idea es explicar los algoritmos que usan cada una de estas técnicas de optimización a medida que las describo, pero quizá sería un poco más organizado si hago otra subsección poniendo ejemplos de los sistemas de AutoML aparte para además no darle tanta importancia a los algoritmos de optimización (hmmm...)
\end{itemize}

Con el objetivo de abordar el problema \ref{cash} el proceso de AutoML consta de tres componentes que definen el proceso de optimización:

\begin{description}
	\item[Espacio de Búsqueda:] precisa los algoritmos y todos los rangos válidos para sus hiperparámetros que son posibles soluciones para un problema de AutoML concreto. Además se pueden optimizar combinaciones complejas de algoritmos, en cuyo caso las restricciones de compatibilidad entre algoritmos también son modeladas.
	\item[Estrategia de Búsqueda:] detalla como se explora el espacio de búsqueda, que puede ser de tamaño exponencial o ilimitado. Se ve afectado por el clásico problema de Exploración vs Explotación, ya que se quieren encontrar soluciones de alto rendimiento rápidamente pero se debe evitar converger prematuramente a regiones sub-óptimas de búsqueda.
	\item[Estrategias de Estimación de Rendimiento:] son mecanismos para estimar la capacidad de las soluciones encontradas por los sistemas de AutoML para predecir en datos nuevos.
\end{description}

La estrategia de búsqueda es el proceso que sustituye la búsqueda de los hiperparámetros realizada por los humanos. Este precedimiento requiere tiempo y recursos considerables debido a los métodos de prueba y error que son necesitados para buscar el mejor modelo y su configuración de hiperparámetros. Por lo tanto, muchos métodos de optimización han surgido con el objetivo de acelerar esta búsqueda para liberar a los humanos de este tedioso proceso y para explorar el espacio de búsqueda definido de forma automática. Este proceso de optimización es el que pretende imitar el rol de los expertos y es el núcleo fundamental para resolver el problema de CASH. Por lo tanto, a continuación se realiza un estudio de los métodos de optimización más usados, ejemplificando su uso en distintos sistemas de AutoML.

% A diferencia de los parámetros del modelo que se aprenden durante el entrenamiento, el científico de datos establece los hiperparámetros del modelo antes de los aspectos de implementación de control y entrenamiento del modelo. Los hiperparámetros se pueden considerar como configuraciones de modelo. Estos deben ajustarse porque los hiperparámetros ideales para un conjunto de datos no serán los mismos en todos los conjuntos de datos.

\subsubsection{Grid Search y Random Search}

\begin{itemize}
	\item[$\checkmark$] Hablar de la necesidad de Grid Search y Random Search como métodos populares para el ajuste de hiperparámetros y del hecho de que son simples métodos de búsqueda que no hacen ninguna suposición sobre el espacio de búsqueda pero son muy ineficientes.
	\item[$\checkmark$] Explicar en lo que consiste Grid Search
	\item[$\checkmark$] Explicar en lo que consiste Random Search.
	\item[$\checkmark$] Compararlos
	\item Poner ejemplos de sistemas que usan RS y GS (casi todos los hacen), hablar quizá de que estos métodos están implementados en sklearn
\end{itemize}

Al ajustar los hiperparámetros de un algoritmo, \textit{grid search} y \textit{radom search} son métodos populares usados por su simplicidad y por el hecho de que no realizan suposiciones sobre el espacio de búsqueda. Cada configuración en el espacio de búsqueda pueden ser evaluado independientemente. Usualmente es ineficiente porque no explota el conocimiento ganado de evaluaciones pasadas.

\textit{Grid Search} (GS) es el proceso de discretizar cada hiperparámetro y evaluar exhaustivamente cada combinación de valores. Los valores numéricos de los hiperparámetros suelen estar espaciados equidistantemente en sus restricciones de cuadro. Para los hiperparámetros categóricos, se consideran un subconjunto o todos los valores posibles. Es la manera más tradicional de ajustar hiper-parámetros. Para obtener la configuración de hiper-parámetros óptima, \textit{grid search} tiene que enumerar cada configuración posible en el espacio de búsqueda, lo que lo hace muy ineficiente. Por ejemplo, la búsqueda de 20 valores de parámetros diferentes para cada uno de los 4 parámetros requerirá 160.000 ensayos de validación cruzada. Esto equivale a 1.600.000 ajustes de modelo y 1.600.000 predicciones si se utiliza una validación cruzada de 10 veces. Es extremadamente costosa tanto en potencia como en tiempo. Además, la discretización es necesaria cuando el espacio de búsqueda es continuo.

Por otro lado, \textit{random search} (RS) o búsqueda aleatoria configura una cuadrícula de valores de hiperparámetros y selecciona combinaciones aleatorias para entrenar el modelo. Esto permite controlar explícitamente el número de combinaciones de parámetros que se intentan. El número de iteraciones de búsqueda se pueden establecer en función del tiempo o los recursos.

RS a menudo tiene un rendimiento mucho mejor que GS en entornos de HPO de mayor dimensión. GS sufre directamente de la maldición de la dimensionalidad, ya que el número requerido de evaluaciones aumenta exponencialmente con el número de hiperparámetros para un número de valores de hiper-parámetros fijo. Esto también parece ser cierto para RS a primera vista, y ciertamente necesitamos un número exponencial de puntos para cubrir bien el espacio. Pero en la práctica, los problemas de HPO a menudo tienen una dimensionalidad efectiva baja: el conjunto de hiperparámetros que influyen en el rendimiento suele ser un pequeño subconjunto de todos los hiperparámetros disponibles. Otra ventaja de RS es que se puede ampliar fácilmente con más muestras; por el contrario, el número de puntos en una cuadrícula debe especificarse de antemano y refinar la resolución de GS posteriormente es más complicado. Todo esto hace que RS sea preferible a GS y una base sorprendentemente sólida para HPO en muchos entornos prácticos. Tanto GS como RS se utilizan e implementan ampliamente en casi todos los marcos de software de ML.

GS y RS son métodos de fácil implementación y unos de los más comunes. Ambos se encuentran implementados en una de las bibliotecas más populares de Python para el aprendizaje de máquinas: \texttt{scikit-learn}. Ambas técnicas evaluan los modelos dado un vector de hiperparámetros determinado usando validación cruzada. Requieren dos argumentos: una instancia del modelo que se quiere optimizar y un espacio de búsqueda definido como un diccionario donde los nombres son los argumentos de los hiperparámetros del modelo y los valores son los valores discretos o en el caso de RS la distribución de valores para muestrear.

Uno de los primeros sistemas de AutoML, Hyperopt, ya usaba RS entre sus estrategias de búsquedas como \textit{baseline}, comparando este método con la Optimmización Bayesiana. En Hyperopt el espacio de búsqueda es una expresión estocástica que siempre evalua a una entrada válida para la función objetivo. Un espacio de búsqueda consiste en expresiones anidadas de funciones, listas o diccionarios. Las expresiones estocásticas son los hiperparámetros. RS está implementada muestrando simplemente estas expresiones estocásticas.

Rafiki es otro de los sistemas de AutoML que usa RS como un método de optimización de hiperparámetros. Rafiki ha sido introducido como un \textit{framework} distribuido que está basado en la idea de que usar modelos previos que logran un buen rendimiento en las mismas tareas. Para la implementación de RS, Rafiki usa un umbral $\alpha$ que representa la probabilidad de elegir una inicialización aleatoria y $1 - \alpha$ representa la probabilidad de usar parámetros pre-entrenados, $\alpha$ se disminuye gradualmente para disminuir la posibilidad de inicialización aleatoria.

Por otro lado, FLAML usa otra versión de RS que ha sido propuesta recientemente, el método de búsqueda directa aleatoria [poner link, está en FLAML], que puede realizar una optimización rentable para la hiperparámetros. El algoritmo utiliza una configuración de bajo costo como punto de partida. En cada iteración, toma muestras de un dirección en una esfera unitaria y luego decide si pasar a una nueva configuración a lo largo de la dirección muestreada (o la dirección opuesta) dependiendo del signo observado en el cambio de error de validación. El tamaño del paso del movimiento se ajusta de forma adaptativa (es grande al principio para acercarse rápidamente a la complejidad requerida) y se reinicia la búsqueda (desde puntos iniciales aleatorizados) ocasionalmente para escapar de los óptimos locales.

\subsubsection{Optimización Bayesiana}


\begin{itemize}
	\item[$\checkmark$] Introducir quizá diciendo su popularidad como técnica de optimización
	\item[$\checkmark$] Hablar de manera general en qué consiste el algoritmo de optimización, hablar de las funciones de adquisición y los modelos sustitutos, especialmente de su forma de equilibrar exploración (la evaluación de tantos conjuntos de hiperparámetros como sea posible) y explotación (asignar más recursos a los hiperparámetros más prometedores)
	\item[$\checkmark$] Hablar de distintos tipos de modelos sustitutos y funciones de adquisicion.
	\item[$\checkmark$] Hablar de los sistemas: \begin{itemize}
		\item Sin meta-learning: AutoWeka, HyperOpt, (\textit{maybe} AutoNet)
		\item Con Meta-learning: 
		\item Meta-features: Auto-Sklearn, SmartML 
		\item Sin mf: Auto-Sklearn 2.0
	\end{itemize}
\end{itemize}

La optimización bayesiana (en inglés \textit{Bayesian Optimization}) (BO) se ha vuelto cada vez más popular como técnica de optimización global para funciones costosas de caja negra, y específicamente para HPO.  

BO es un algoritmo iterativo, cuya idea clave es modelar el mapeo entre un conjunto de hiperparámetros $\lambda$ y da como resultado una estimación de su rendimiento $\hat{c}(\lambda)$ ($\lambda \rightarrow \hat{c}(\lambda)$) basado en valores de rendimiento observados encontrados en un archivo $A$ mediante regresión (no lineal). Este modelo aproximado se denomina modelo sustituto, o modelo probabilístico, para el cual es normalmente utilizado un proceso gaussiano o un bosque aleatorio. BO comienza en un archivo $A$ lleno de configuraciones evaluadas. Este archivo es normalmente generado a partir de una configuración de diseño inicial, típicamente muestreada al azar. BO luego usa el archivo para ajustar el modelo sustituto, que para cada $\lambda$ produce una estimación del rendimiento $\hat{c}(\lambda)$, así como una estimación de la incertidumbre del modelo $\hat{\sigma}(\lambda)$. Sobre la base de estas predicciones, BO establece un función de adquisición $u(\lambda)$ fácil de evaluar, cuyo resultado codifica una compensación entre explotación (el modelo sustituto predice que el candidato $\lambda$ tiene un valor $c$ bueno y bajo) y exploración (el modelo sustituto es muy incierto acerca de la $c(\lambda)$, probablemente porque el área circundante no ha sido exploradam minuciosamente).

\quad

\textbf{Modelos sustituos}

\quad

La elección del modelo sustituto tiene una gran influencia en el rendimiento de BO y a menudo está relacionado con las propiedades del espacio de búsqueda $\hat{\varLambda}$. 

Si $\hat{\varLambda}$ es puramente numérico, los procesos guasianos (GPs), son los usados más a menudo. Extensiones para los espacios jerárquicos mezclados que son basados en kernels especiales existen, y el uso de \emph{embeddings} aleatorios ha sido sugerido para los espacios de grandes dimensiones. Más importante, GPs tiene una complejidad de tiempo de ejecución que es cúbica en el número de ejemplos, lo que puede resultar en una sobrecarga significativa cuando el archivo $A$ se vuelve grande.

Los bosques aleatorios (\emph{random forests}), utilizados más notablemente en SMAC (\emph{Sequential Model-based Algorithm Configuration}), también han mostrado buenos rendimientos como modelos sustitutos para BO. Su ventaja es su habilidad nativa para manejar hiperparámetros discretos y, con modificaciones menores, incluso los hiperparámetros dependientes sin la necesidad de preprocesamiento. Las implementaciones estándar de bosque aleatorio todavía pueden manejar hiperparámetros dependientes al tratar los valores no factibles como faltantes y realizar la imputación. Los bosques aleatorios tienden a funcionar bien con archivos más grandes e introducen menos gastos generales que los GPs. SMAC usa la desviación estándar de las predicciones de los árboles como una estimación de incertidumbre heurística $\hat{\sigma}(\lambda)$,  sin embargo, existen alternativas más sofisticadas para proporcionar estimaciones no sesgadas. Dado que los árboles no son modelos espaciales basados en la distancia, el estimador de incertidumbre no aumenta cuanto más extrapolamos de los puntos de entrenamiento observados. Este podría ser una explicación de por qué GP supera a los sustitutos basados en árboles en la búsqueda en espacios puramente numéricos.

Las redes neuronales o \emph{Neural Networks} (NN), han mostrado un buen rendimiento en particular con espacios de entrada no triviales, por lo que se consideran cada vez más como modelos sustitutos de BO. Los NN ofrecen eficientes y versátiles implementaciones que permiten el uso de gradientes para una optimización más eficiente de la función de adquisición. Los límites de incertidumbre en las predicciones se pueden obtener, por ejemplo, utilizando Redes Neuronales Bayesianas que combinan Redes Neuronales con un modelo probabilístico de los pesos de la red o regresión de base adaptativa donde solo se agrega un regresor lineal bayesiano a la última capa de la NN.

\quad

\textbf{Función de adquisición}

\quad

La función de adquisición equilibra la predicción del modelo sustituto $\hat{c}(\lambda)$ y su incertidumbre posterior $\hat{\sigma}(\lambda)$ para asegurar tanto la exploración de las regiones inexploradas de $\hat{\varLambda}$, así como la explotación de las regiones que han tenido un buen rendimiento en las evaluaciones previas.

Diferentes funciones de adquisición han sido diseñadas para la optimización bayesiana: \textit{probability of improvement} (probabilidad de mejora), \textit{entropy search} (búsqueda de entropía), \textit{upper confidence bound} (límite de confianza superior), \textit{lower confidence bound} (límite de confianza inferior), \textit{expected improvement} (mejora esperada), siendo esta última una de las más populares.

Basada en teoría de probabilidades básica, esta puede ser calculada relativa a la estimación actual del rendimiento óptimo. Suponiendo que nuestra métrica de rendimiento debería ser maximizada (por ejemplo, \textit{accuracy}, \textit{precission}, recobrado, etc), que para cualquier ajuste de combinación de hiperparámetros $\lambda$ tenemos la media predicha y el error estándar de esa métrica ($\mu(\lambda)$ y $\sigma(\lambda)$ respectivamente) y de los datos anteriores el mejor valor (medio) de rendimiento fue $m_{opt}$ \textit{expected improvement} es determinado usando:

$$
 EI(\lambda, m_{opt}) = \delta(\lambda)\Phi\left(\frac{\delta(\lambda)}{\sigma(\lambda)}\right) + \sigma(\lambda)\phi\left(\frac{\delta(\lambda)}{\sigma(\lambda)} \right)
$$

Donde $\delta(\lambda) = m_{opt} - \mu(\lambda)$, la función  $\Phi(\cdot)$ es la función de distribución acumulativa y $\phi(\cdot)$ es la densidad normal estándar.

El valor de $\delta(\lambda)$ mide cuán cerca estamos (en promedio) al mejor valor actual de rendimiento. Cuando se necesitan nuevos parámetros de ajuste candidatos, el espacio de $\theta$ es buscado por el valor que maximiza la mejora esperada.

\quad

\textbf{Ejemplos}

\quad


Esta estrategia de búsqueda ha sido ampliamente usada en diversos sistemas de AutoML. Auto-WEKA \cite{thornton2013auto}, es uno de los primeros sistemas de AutoML, introduciendo el problema de CASH. Esta herramienta utiliza el enfoque de optimización bayesiana para la optimización de los hiperparámetros. En particular, usa \textit{Sequential Model-Based Optimization} (SMBO) o la Optimización Secuencial Basada en Modelos, un sistema versátil de optimización estocástica que puede trabajar explícitamente con hiperparámetros categóricos y continuos, y que pueden explotar la estructura jerárquica derivados de los hiperparámetros condicionales. Auto-WEKA implementa dos algoritmos de SMBO que son adecuados para la tarea de la selección combinada de modelos y optimización de hiperparámetros: \textit{Sequential Model-based Algorithm Configuration} (SMAC) y \textit{Tree-structured Parzen Estimator} (TPE). Empíricamente compararon estos dos optimizadores para buscar en el espacio de hiperparámetros de 786 dimensiones de Auto-WEKA y terminaron recomendando la variente basada en el método de optimización bayesiana SMAC.

Hyperopt \cite{bergstra2013hyperopt} y Auto-Sklearn \cite{fuerer2015efficient} fueron dos sistemas posteriores de AutoML que también utilizan dos optimizadores bayesianos: SMAC y TPE respectivamente. Además de abordar el problema de CASH con SMAC, Auto-Sklearn además emplea meta-learning para empezar la optimización bayesiana con buenas configuraciones [link del paper de Auto-Sklearn de meta-learning] evaluada en datasets similares. Auto-Sklearn además constuye \textit{ensembles} de los modelos evaluados con SMAC con la técnica de la selección de \textit{ensemble} de [poner link] para reducir el riesgo de hacer \textit{overfitting}. 

La optimización bayesiana ha sido también utilizada para incluir redes neuronales modernas. Uno de las primeras herramientas en llenar este vacío fue Auto-Net \cite{mendoza2016towards}, un sistema que automáticamente configura redes neuronales con SMAC siguiendo el mismo enfoque de AutoML que Auto-WEKA y Auto-Sklearn. Para la primera version de Auto-Net este sistema está implmentado dentro de Auto-Sklearn añadiendo un nuevo componente de clasificación y regresión. Auto-Net está limitado a las redes neuronales complementamente conectadas y hace uso de la biblioteca de Python \textit{Lasagne}, la cual está construida sobre Theano.


Otro sistema que usa BO para abordar el problema de NAS es Auto-Keras \cite{jin2019auto}. En esta herramienta se usa la optimización bayesiana para guiar el morfismo de la red para una búsqueda efectiva de arquitecturas neuronales. Como modelos sustitutos usan procesos gausianos (GPs) y como funciones de adquisición \textit{Upper Confidence Bound} (límite de confianza superior).

\subsubsection{Multifidelity Optimization}


\begin{itemize}
	\item[$\checkmark$] Introducir este concepto mediante su necesidad de uso para las redes neuronales.
	\item[$\checkmark$] Explicar de forma concreta en que consiste el método de optimización.
	\item[$\checkmark$] Hablar de AutoPytorch como ejemplo de sistema de AutoML (usa mtl sin meta-features)
\end{itemize}

La optimización bayesiana es popular para optimizar los objetivos de caja negra que consumen mucho tiempo. No obstante, para el ajuste de hiperparámetros en redes neuronales profundas, el tiempo necesario para evaluar el error de validación incluso para unos pocos ajustes de hiperparámetros sigue siendo muy costosa. La optimización de multifidelity promete alivio al usar \emph{proxies} más baratos para tales objetivos, por ejemplo, error de validación para una red entrenada usando un subconjunto de puntos de entrenamiento o menos iteraciones de las requeridas para la convergencia. (poner referencia del paper de \textit{multifidelity optimization})

El concepto de \textit{Multifidelity} en HPO se refiere a todos los enfoques de ajuste que pueden manejar de manera eficiente a un learner $I(D, \lambda)$ con un hiperparámetro de presupuesto $\lambda_{budget}$ como un componente de $\lambda$ que influye en el costo computacional del procedimiento de ajuste de una manera monótonamente creciente. Los valores de $\lambda_{budget}$ más altos implican un tiempo de ejecución más largo del ajuste. Suponemos conocer las restricciones de $\lambda_{budget}$ en forma de límite inferior y superior. Por lo general, asumimos que más presupuesto es mejor en términos de rendimiento predictivo (pero, naturalmente, más caro), pero el \textit{overfitting} puede ocurrir en algún momento. Además, suponemos que la relación entre el presupuesto y el rendimiento de la predicción cambia de forma algo suave, por lo que al evaluar múltiples configuraciones de hiperparámetros con un presupuesto $\lambda$ pequeño, esto proporciona al menos una indicación con respecto a su clasificación cuando se evalúa en el presupuesto completo. Normalmente, esto implica un procedimiento de ajuste secuencial, donde $\lambda_{budget}$ es, por ejemplo,  el número de pasos de descenso de gradiente (estocásticos) o el número de miembros del conjunto agregados (de refuerzo) secuencialmente. Otra opción, de aplicación general, es realizar una submuestra de los datos de formación del 0\% al 100\% antes de la formación y tratar esto como control de presupuesto. Los algoritmos HPO que explotan dicho parámetro $\lambda_{budget}$, generalmente gastando el presupuesto en configuraciones de hiperparámetros baratos antes para la exploración y luego concentrándose en los más prometedores más tarde, se denominan métodos de multifidelity.

El ejemplo más notable del uso de esta estrategia es Auto-PyTorch \cite{zimmer2021auto}. \textit{Auto-PyTorch Tabular} es un sistema de AutoML dirigido a los datos tabulares que realizan optimización  multifidelity en un espacio conjunto de parámetros de arquitectura y hiperparámetros de entrenamiento para redes neuronales. Auto-PyTorch Tabular es el sucesor de Auto-Net y combina varios enfoques del estado del arte, desde optimización multifidelity a \textit{ensemble learning} y meta-learning para una selección impulsada por los datos para la selección de configuraciones iniciales para inicializar la optimización.

\subsubsection{Algoritmos Evolutivos}


\begin{itemize}
	\item[$\checkmark$] Introducción de los algoritmos evolutivos
	\item[$\checkmark$] Definir conceptos importantes como \textit{individuos}, \textit{población} y sus principales operaciones \textit{mutation} y \textit{crossover}. Explicarlos en la terminología de HPO
	\item[$\checkmark$] Hablar un poco de sus desventajas y propiedades.
	\item[$\checkmark$] Mencionar los distintos paradigmas de los algoritmos evolutivos, para hacer incapié en varios usados en los sistemas de AutoML: GGA, GA, PGE
	\begin{itemize}
	\item[$\checkmark$] Al explicar estos algoritmos concretos, hablar de los distintos sistemas de AutoML
	\item[$\checkmark$] EA: Autostacker 
	\item[$\checkmark$] GP: TPOT. No tiene mtl, pero hay trabajos incorporándolo, explicarlos.
	\item[$\checkmark$] GGA: Auto-MEKA\_GPP, RECIPE (sin mtl)
	\item PGE: Auto-GOAL sin mtl
	\end{itemize}
\end{itemize}

Un algoritmo evolutivo (EA) es un subconjunto de la computación evolutiva, un algoritmo genérico de optimización metaheurística basado en la población. Un EA utiliza mecanismos inspirados en la evolución biológica, como la reproducción, la mutación, la recombinación y la selección. Las soluciones candidatas al problema de optimización juegan el papel de individuos en una población, y la función de aptitud determina la calidad de las soluciones. La evolución de la población tiene lugar luego de la aplicación repetida de los operadores anteriores.

En un algoritmo evolutivo, una \textit{población} de soluciones candidatas (llamadas individuos, criaturas o fenotipos) a un problema de optimización evoluciona hacia mejores soluciones. Cada solución candidata tiene un conjunto de propiedades (sus cromosomas o genotipo) que se pueden mutar y alterar.

La evolución generalmente comienza a partir de una población de individuos generados aleatoriamente y es un proceso iterativo, con la población en cada iteración llamada \textit{generación}. En cada generación, se evalúa la aptitud de cada individuo de la población; la aptitud suele ser el valor de la función objetivo en el problema de optimización que se resuelve. Los individuos más aptos se seleccionan estocásticamente de la población actual, y el genoma de cada individuo se modifica (recombina y posiblemente muta al azar) para formar una nueva generación. La nueva generación de soluciones candidatas se utiliza luego en la siguiente iteración del algoritmo. Por lo general, el algoritmo termina cuando se ha producido un número máximo de generaciones o se ha alcanzado un nivel de aptitud satisfactorio para la población.

En la terminología de la optimización de hiperparámetros un \textit{individuo} es una configuración de hiperparámetros única, la \textit{población} es un conjunto de configuraciones de hiperparámetros actualmente mantenido y la \textit{aptitud} de un individuo es su error de generalización $c(\lambda)$. La mutación es el cambio (aleatorio) de uno o unos pocos valores de hiperparámetros en una configuración. El cruce crea una nueva configuración de hiperparámetros mezclando (aleatoriamente) los valores de otras dos configuraciones.

Los EA estaban limitados a espacios numéricos en su formulación original, pero pueden extenderse fácilmente para manejar espacios mixtos tratando componentes de diferentes tipos de forma independiente, por ejemplo, agregando un valor aleatorio normalmente distribuido a hiperparámetros de valor real mientras se agrega la diferencia de dos geométricamente valores distribuidos a hiperparámetros con valores enteros. Al definir operaciones de mutación y cruce que operan en estructuras de árbol o gráficos, incluso es posible realizar la optimización de \emph{pipelines} de preprocesamiento o arquitecturas de redes neuronales utilizando algoritmos evolutivos.

Los algoritmos evolutivos a menudo funcionan bien al aproximar soluciones a todo tipo de problemas porque, idealmente, no hacen ninguna suposición sobre la función de aptitud subyacente. % En la mayoría de las aplicaciones reales de EA, la complejidad computacional es un factor prohibitivo. De hecho, esta complejidad computacional se debe a la evaluación de la función de aptitud. La aproximación al fitness es una de las soluciones para superar esta dificultad.

Un ejemplo de uso de los algoritmos evolutivos se encuentra en Autostacker \cite{chen2018autostacker}. Autostacker combina una arquitectura innovadora jerárquica de stacking y EA para realizar la búsqueda de parámetros de forma eficiente. Encuentra combinaciones innovadoras y estructuras de modelos de \textit{machine learning} en vez de seleccionar un solo modelo y optimiza sus hiperparámetros. Para esto está inspirado en el método \textit{stacking} de \textit{ensemble learning}. Por otro lado, EA les permite encontrar buenas soluciones en un espacio grande de variables. Además, explotando la naturaleza paralela de los algoritmos evolutivos, Autostacker encuentra rápidamente buenos candidatos de pipelines.

Por otro lado, TPOT \cite{olson2019tpot} optimiza flujos de algoritmos mediante una versión de programación genética (\textit{genetic programming}), una técnica muy conocida en la programación evolutiva para la construcción automática de programas. En la programación genética las soluciones están en la forma de programas de computadoras, y su aptitud está determinada por su habilidad para resolver un programa computacional. Una diferencia en el espacio de búsqueda de TPOT, en comparación con el resto de los sistemas mencionados, es que permite el uso de muchas copias de un dataset, el cual es procesado en paralelo por diferentes métodos de pre-procesamiento y combinados con posterioridad. Además, TPOT considera una selección Pareto (NSGA-II) \cite{NSGA-II} para realizar la búsqueda multiobjetivo. Dos objetivos separados son considerados: maximizar la medida de precisión del flujo de trabajo y minimizar la complejidad total del mismo, que está dado por el números de operadores del flujo de trabajo, con el objetivo de evitar \textit{overfitting}.

Otro ejemplo lo constituye RECIPE \cite{de2017recipe}, el cual supera las debilidades de los otros \textit{frameworks} basados en algoritmos evolutivos anteriores, tales como generación de individuos inválidos, y organiza un gran número de posibles métodos de pre-procesamiento de datos y de clasificación adecuados en una gramática. Para esto hace uso de un enfoque de Programación Genética basada en Gramáticas (GPP). Esta gramática permite dirigir la búsqueda, restringiendo las operaciones de mutación y cruzamiento. La estructura de la gramática permite a RECIPE evitar, a diferencia de TPOT, la generación de flujos inválidos, y puede acelerar la búsqueda.

Otra herramienta de AutoML que usa GPP como estrategia de búsqueda es $Auto-MEKA_{GPP}$ \cite{de2018automated}. $Auto-MEKA_{GPP}$ está diseñada para resolver el problema de \textit{Multi-Label Classification} (MLC) y se apoya en la herramienta MEKA, la cual ofrece un número de algoritmos de MLC. En MLC, cada ejemplo puede ser asociado con una o más etiquetas de clase, lo que hace los problemas de MLC más difíciles que los problemas de clasificación convencionales, que poseen una sola etiqueta (\textit{Single Label Classification}, SLC). Mediante GPP, $Auto-MEKA_{GPP}$ es capaz de manejar la naturaleza jerárquica compleja del espacio de búsuqeda de MLC mientras evita la generación de soluciones inválidas. Sin embargo, en el trabajo se enfocan en la búsqueda de algoritmos y sus hiperparámetros, no en flujos de trabajo completos. Esto se debe a que el espacio de búsqueda de MLC es mucho más grande que el espacio de búsqueda de SLC.

\begin{itemize}
	\item AUTOGOAL
\end{itemize}

\subsubsection{Aprendizaje por refuerzo}

\begin{itemize}
	\item[$\checkmark$] Introducir este método como algoritmo de optimización
	\item[$\checkmark$] Hablar de la retroalimentación retardada: las retroalimentaciones (la recompensa y el estado) no necesitan ser devueltos inmediantamente una vez que se toma una acción pero una arquitectura solo se puede evaluar después de que se componga toda su estructura
	\item[$\checkmark$] Explicar su desventaja de que consume gran cantidad de datos
	\item[$\checkmark$] Hablar de los sistemas que usan Reinforcement Learning: AlphaD3M (mtl con meta-features)
\end{itemize}

El aprendizaje por refuerzo (RL), como estrategia de búsqueda, consiste en entrenar un agente que realiza modificaciones sobre una solución con el objetivo de maximizar una recompensa que depende del rendimiento de dicha solución. Es un marco de optimización muy general y sólido, que puede resolver problemas con retroalimentación retardada. A diferencia de los métodos anteriores, las retroalimentaciones (es decir, la recompensa y el estado) no necesitan ser devueltos inmediatamente una vez que se toma una acción. Se pueden devolver después de realizar una secuencia de acciones. 

Sin embargo, el rendimiento de una arquitectura solo se puede evaluar después de que se componga toda su estructura, lo que implica una recompensa retrasada. Por lo tanto, la generación de arquitectura iterativa sigue naturalmente la propiedad de RL. 

Debido a las retroalimentaciones retrasadas, AutoML con aprendizaje por refuerzo consume una gran cantidad de datos y es necesario explorar métodos más eficientes. Algunos esfuerzos actuales para abordar estos problemas son el aprendizaje de arquitecturas transferibles a partir de conjuntos de datos más pequeños y la reducción del espacio de búsqueda compartiendo parámetros.

Esta estrategia es usual en la búsqueda de arquitecturas de redes neuronales (NAS), donde el agente puede realizar acciones como añadir, quitar o modificar una capa, o sus hiperparámetros. Las propuestas difieren en cómo representan la política del agente, y como la optimizan.

Un ejemplo del uso de esta estrategia se encuentra en Alpha3DM \cite{drori2018alphad3m}. Alpha3DM es un sistema de aprendizaje automático basado en \textit{meta reinforcement learning} usando secuencia de modelos con \textit{self play}. Inspirado en AlphaZero \cite{alphazero}, el jugador iterativamente construye un pipeline seleccionando entre un conjunto de acciones entre las cuales se encuentran inserción, eliminación y reemplazo de partes del pipeline. Una ventaja inherente de este enfoque es que al final del proceso, una vez haya un pipeline completo, es completamente explicable, incluidas todas las acciones y decisiones que llevaron a esa síntesis. Alpha3DM usa una red neuronal (específicamente, un LSTM)  para predecir el rendimiento de un pipeline y las probabilidades de acción, junto con un Monte-Carlo Tree Search (MCTS), el cuál toma decisiones fuertes basados en la red.

\subsubsection{Multi-Armed Bandit}


\begin{itemize}
	\item[$\checkmark$] Introducir el problema de MAB, mencionar que es un caso particular de RL
	\item[$\checkmark$] Explicar la solución del problema en el contexto de la optimización de hiperparámetros
	\item Hablar de los sistemas que usan MAB:
	\begin{itemize}
		\item ATM usa BO y MAB Híbrido (explicar como cambia el algoritmo). Usa meta-learning
		\item Alpine Meadow usa cost-based MAB con BO (explicar como cambia el algoritmo). Usa meta-learning con meta-features
	\end{itemize}
\end{itemize}

\textit{Multi-Armed Bandit} (MAB) es un problema en el que se debe asignar un conjunto limitado fijo de recursos entre opciones en competencia (alternativas) de una manera que maximice su ganancia esperada, cuando las propiedades de cada opción se conocen solo parcialmente en el momento de la asignación, y pueden entenderse mejor a medida que pasa el tiempo o mediante la asignación de recursos a la elección. Este es un problema clásico de aprendizaje por refuerzo que ejemplifica el dilema de compensación de exploración-explotación.

El problema de MAB puede definirse de la siguiente forma en el contexto de HPO:

\begin{definition}

Dado un conjunto de acciones $a \in A$, un presupuesto de tiempo $T$ y las distribuciones de recompensa $D$ que son desconocidas e independientes, en cada ronda $t \in [T]$:

\begin{enumerate}
	\item Un algoritmo elige un brazo $a \in A$
	\item Los algoritmos observan una recompensa del brazo elegido $a_t$
\end{enumerate}

Encontrar el algoritmo que mejor aproxima la solución con la menor pérdida de recompensa.

\end{definition}

Específicamente, cada brazo $a_t$ es asociado con una configuración de hiperparámetros $\lambda$ y la recompensa (estocástica) por seleccionar (alar) un conjunto de hiperparámetros previo está definido en términos de error en las predicciones.

ATM \cite{swearingen2017atm} constituye en ejemplo de esta estrategia. ATM es un servicio de AutoML multi-usario que puede ser hosteado en cualquier infraestructura, ya sea la nube o un \textit{cluster}. En ATM un usuario puede decidir entre dos métodos de optimización: 1) un sistema de optimización híbrida bayesiana y \textit{multi-armed bandit} y 2) un sistema de recomendación de modelos que funciona explotando las técnicas de modelación de los rendimientos anteriores en una gran variedad de dataset. El aprendizaje \textit{bandit} empleado en este sistema modela cada hiperpartición, el cual define un conjunto ajustable de hiperparámetros condicionales, como un brazo $a$. Dado que cada brazo, cuando es elegido, proporciona una recompensa aleatoria elegida de la distribución subyacente, el propósito del problema MAB es decidir cuál brazo alar para maximizar la recompensa a largo plazo, reevaluando la decisión después de que cada recompensa es observada. En el contexto de ATM, cada hiperpartición es tratada como un brazo con su propia distribución de recompensas. A medida que pasa el tiempo, aprende más sobre la distribución, y balancea exploración y explotación eligiendo las hiperparticiones más prometedoras para formar los clasificadores. Además, existen estrategias de memoria para cambiar la formulación del problema cuando las redistribuciones de las recompensas son no-estacionarias. El algoritmo de ATM asume que las distribuciones subyacentes de cada brazo es estacionario. En el contexto de ATM, durante la optimización del modelo, a medida que el modelo sustituto de BO continua aprendiendo sobre las hiperparticiones y es capaz de descubrir parametrizaciones que continuan teniendo una recompensa cada vez mejor, esencialmente cambia la percepción de la distribución de recompensa. La optimización bayesiana es usada dentro de cada hiperpartición, usando una técnica de meta-modelado basado en Procesos Gausianos (GPs) para identificar los mejores hiperparámetros dado el rendimiento de los clasificadores ya construidos para esa hiperpartición. ATM soporta 2 funciones de adquisición: \textit{Expected Improvement} y \textit{Expected Improvement per Time}[esto lleva link que está en el paper].

\textit{Alpine Meadow} \cite{shang2019democratizing} ha sido recientemente introducido como una herramienta \emph{interactiva} de AutoML. Además de su interoactividad se destaca por su enfoque de diseño combinadamente sistemático y algorítmico. Por un lado se aprovechan de ideas de \textit{optimización de consultas} y por otro lado muestran estrategias nuevas y de poda combinando \textit{Multi-Armed Bandits} basado en costo y Optimización Bayesiana. En particular, usa un mecanismo de meta-learning para proveer al usario con una recomendación de un modelo inicial, usándolo además como retroalimentación. \textit{Multi-Armed Bandits} es usado para modelar el proceso de selección de un flujo de trabajo general basado en la historia. La idea principal es la siguiente: 
\begin{itemize}
	\item Inicialización: tiene un brazo para cada pipeline general relacionados (basado en la tarea y el dataset) ejecutados en el pasado y presentan una puntuación para cada brazo basado en la experiencia pasada.
	\item Selección: Seleccionamos un brazo (es decir, pipeline) para ejecutar aleatoriamente pero de forma proporcional a la puntuación. Cuando la ejecución se haya terminado,
	\item Guardar Historia: Guardar el resultado en el registro de la historia.
	\item Ajustar Puntuaciones: Ajustamos las puntuaciones consecuentemente, y el proceso se repite en 1
\end{itemize}

 % Toma prestado de la optimización de consultas la creación de espacios de búsqueda basado en reglas

\subsubsection{Monte Carlo Tree Search (MCTS)}


\begin{itemize}
	\item[$\checkmark$] Introducir el algoritmo
	\item[$\checkmark$] Explicar los pasos en los que consite MCTS: selección, expansión, simulación y backpropagation.
	\item[$\checkmark$] Mencionar como MCTS extiende el algoritmo de MAB [link de mosaic]
	\item[$\checkmark$] Ejemplificar la adaptación del procedimiento estándar de MCTS al problema de HPO 
	\item[$\checkmark$] Poner ejemplos de sistemas
	\item MOSAIC: usa meta-learning con meta-features
\end{itemize}

Monte Carlo Tree Search (MCTS) es un algoritmo de búsqueda heurística para algunos tipos de procesos de decisión, más notablemente aquellos empleados en software que juegan juegos de mesa. El enfoque de MCTS es en el análisis de los movimientos más prometedores, expandiendo el árbol de búsqueda basado en un muestreo aleatorio del espacio de búsqueda. Cada ronda en MCTS consiste en 4 pasos:
\begin{description}
	\item[Selección:] comienza desde la raíz \textit{R} y selecciona los nodos secundarios sucesivos hasta que se alcance un nodo hoja \textit{L}. La raíz es el estado actual del juego y una hoja es cualquier nodo que tenga un hijo potencial a partir del cual aún no se ha iniciado ninguna simulación (reproducción). La siguiente sección dice más sobre una forma de sesgar la elección de los nodos secundarios que permite que el árbol del juego se expanda hacia los movimientos más prometedores, que es la esencia de la búsqueda del árbol de Monte Carlo.
	\item[Expansión:] a menos que \textit{L} termine el juego de manera decisiva (por ejemplo, ganar/perder/empatar) para cualquiera de los jugadores, crear uno (o más) nodos secundarios y elija el nodo \textit{C} de uno de ellos. Los nodos secundarios son cualquier movimiento válido desde la posición del juego definida por \textit{L}.
	\item[Simulación:] Completa una reproducción aleatoria del nodo $C$. Este paso a veces también se denomina $playout$ o $rollout$. Un \textit{playout} puede ser tan simple como elegir movimientos aleatorios uniformes hasta que se decida el juego (por ejemplo, en el ajedrez, el juego se gana, se pierde o se empata).
	\item[\textit{Backpropagation}:] usa el resultado del \textit{playout} para actualizar la información en los nodos en la ruta de \textit{C} a \textit{R}. 
\end{description}

MCTS extiende el algoritmo de \textit{Multi-Armed Bandit} (MAB) a espacios de búsqueda estructurados como árboles. Un problema MAB define un problema de RL con un solo estado. MCTS relaja esta limitación de un solo estado y es conocido por ser un enfoque de RL eficiente \cite{rakotoarison2019automated}

Un ejemplo de adaptación del procedimiento estándar de MCTS al problema de HPO es el llevado a cabo por MOSAIC \cite{rakotoarison2019automated} que sigue la siguiente estrategia:

\begin{itemize}
	\item El camino del árbol del nodo raíz a un node interno representa una solución parcial
	\item  En cada nodo no-terminal, la elección del nodo hijo es realizado usando la regla UCB (\textit{Upper Confidence Bound}) en el caso finito, seleccionando el nodo hijo que maximiza: 
	$$
	 \mu_i + c\sqrt{\frac{logN}{n_i}}
	$$
	donde $\mu_i$  es el valor del nodo $i$, $N$ es el número de veces que el nodo padre fue visitado, $n_i$ es el número de veces que el nodo $i$ es visitado, y $c$ es la constante dependiente del problema que equilibra exploración y explotación.
	\item Cuando se alcanza los límites del árbol visitado, una estrategia de \textit{rollout} es aplicada hasta alcanzar un nodo terminal y calcular la recompensa asociada.
	\item La recompensa asociada al camino completo del árbol (nodo terminal, solución completa \textit{m}) es calculada.
	\item Después de la evaluación del nodo terminal, la recompensa es \textit{backpropagated} para actualizar el valor en cada nodo visitado del camino del árbol; este valor apoyará la elección entre los nodos hijos en el siguiente recorrido por el árbol.
\end{itemize}

\subsubsection{Descenso por Gradientes}

\begin{itemize}
	\item[$\checkmark$] Introducir el método hablando de los problemas de su uso para HPO
	\item[$\checkmark$] Explicar cómo se usa en la solución del problema
	\item[$\checkmark$] Hablar de HyperGD, y poner el ejemplo de Oracle, que es un sistema que usa este método de optimización. Usa meta-learning
\end{itemize}

Los problemas de HPO suelen ser complejos ya que en general el objetivo no suele ser diferenciable o incluso continuo. Cada hiperparámetro que está definido por un conjunto discreto introduce superficies no suaves. Además en general el problema de HPO no es convexo. Toda la teoría de convergencia de Gradient Descent  (GD) asume, que el problema subyacente es convexo. Por lo tanto, el descenso por gradientes no es tan popular como los métodos anteriores. Sin embargo, centrándose en alguna función de pérdida diferenciable, por ejemplo, \textit{square loss} y \textit{logistic loss}, los hiperparámetros continuos se pueden optimizar mediante el descenso de gradiente.

En comparación con los métodos anteriores, los gradientes ofrecen la información más precisa donde se ubican las mejores configuraciones. A diferencia de los problemas de optimización tradicionales cuyos gradientes se pueden derivar explícitamente a partir del objetivo, en los problemas de AutoML, los gradientes deben calcularse numéricamente.

Por lo general, esto se puede hacer con métodos de diferenciación finita pero con costos elevados. Para algunos métodos tradicionales de aprendizaje automático, por ejemplo, regresión logística y SVM, se propone el gradiente aproximado para buscar hiperparámetros continuos. El cálculo de gradientes exactos se basa en la convergencia del entrenamiento del modelo. A través de un gradiente inexacto, los hiperparámetros se pueden actualizar antes de que converja el entrenamiento del modelo, lo que hace que el método de descenso del gradiente sea más eficiente.

Otra forma de calcular gradientes es mediante el aprendizaje reversible [poner link del artículo sobre reverse learning] (también denominado diferenciación automática). Calcula gradientes con la regla de la cadena, que también se utiliza en el proceso de retropropagación del entrenamiento de la red. Se ha aplicado en la búsqueda de hiperparámetros de aprendizaje profundo. 

Oracle AutoML~\cite{OracleAutoML} es un ejemplo de esta estrategia de búsqueda. Para la optimización de hiperparámetros usa específicamente HyperGD, que está basado en gradientes altamente paralelos que realizan una optimización asincrónica en diferentes dimensiones de hiperparámetros en paralelo. La característica más destacada de Oracle AutoML es que tiene un pipeline de AutoML libre de iteraciones, diseñado no solo para proveer modelos precisos, sino también en un tiempo de ejecución más pequeño. Para liberar las etapas del aprendizaje automático de iteraciones hacen uso de modelos proxy, que son meta-aprendidos, que predicen el rendimiento relativo de algoritmos y subconjuntos de datos. Además, selecciona una muestra representativa de un dataset, junto con la fila y las dimensiones de las características, optimizadas por un algoritmo seleccionado. La reducción de datos adaptativos acelera la optimización de hiperparámetros con un impacto mínimo en el rendimiento predictivo de los modelos.

\subsubsection{Filtrado Colaborativo}

\begin{itemize}
	\item[$\checkmark$] Introducir este método, quizá hablando de cómo no es una técnica de optimización pero se usa para la selección de modelos
	\item[$\checkmark$] Explicar en que cosiste este método
	\item[$\checkmark$] Explicar el método en el contexto de HPO
	\item Hablar de los sistemas que usan filtrado colaborativo
	\begin{itemize}
		\item OBOE: Usa meta-learning sin meta-features
		\item PMF: Usa CF con BO (explicar cómo se modifica el algoritmo). Usa Meta-Learning sin meta-features
%		\item Describir de CofiRank, un algoritmo particular de filtrado colaborativo que es una alternativa de MMMF (\textit{Maximum Margin Matrix Factorization}) para hablar de ActivMetaL. Este sistema usa meta-learning sin meta-features
	\end{itemize}
\end{itemize}

El filtrado colaborativo es un método para realizar predicciones automáticas (filtrado) sobre los intereses de un usuario mediante la recopilación de preferencias o información sobre gustos de muchos usuarios (colaborando). El supuesto subyacente del enfoque de filtrado colaborativo es que si una persona A tiene la misma opinión que una persona B sobre un tema, es más probable que A tenga la opinión de B sobre un tema diferente que la de una persona elegida al azar. Por ejemplo, un sistema de recomendación de filtrado colaborativo para las preferencias en la programación de televisión podría hacer predicciones sobre qué programa de televisión le gustaría a un usuario dada una lista parcial de los gustos de ese usuario (gustos o disgustos). Esto difiere del enfoque más simple de dar una puntuación media (no específica) para cada elemento de interés, por ejemplo, en función de su número de votos.

En HPO podemos realizar una analogía entre los datasets (usuarios) que proveen calificaciones $P_{i,j}$, para las configuraciones de hiperparámetros $\lambda_i$, y técnicas de factorización son usadas para predecir los valores desconocidos $P_{i,j}$ y recomendar las mejores configuraciones para cualquier tarea. Un problema importante es el \textit{problema de arranque en frío}, ya que la factorización de matrices requiere al menos algunas evaluaciones en $\lambda_{new}$.

A modo de ejemplo, OBOE \cite{yang2018oboe} empieza construyendo una \textit{matriz de error} para un conjunto base de algoritmos de machine learning, donde cada fila representa un dataset y cada columna representa un algoritmo de machine learning. Cada celda en la matriz representa el rendimiento de un modelo particular de machine learning con sus hiperparámetros en un dataset específico. El sistema tiene en cuenta dos problemas importantes: (1) inicializaciones con restricciones de tiempo: cómo elegir un modelo prometedor inicial bajo restricciones de tiempo y (2) \textit{Active learning}: cómo mejorar la predicción inicial dando más recursos computacionales. Para encontrar el mejor algoritmo de machine learning para un nuevo dataset, OBOE ejecuta un conjunto particular de modelos correspondientes a un subconjunto de columnas en la matriz de error, las cuales son predecidas para ejecutar eficientemente en el nuevo dataset. Para encontrar el resto de las entradas en la fila, el rendimiento de los modelos que no han sido evaluados son predecidos. Lo mejor sobre este enfoque es que infiere el rendimiento de un montón de modelos sin la necesidad de ejecutarlos o incluso computar sus meta-features  y esa es la razón por la cual OBOE puede encontrar modelos que funcionan bien con un presupuesto de tiempo razonable.

Por otro lado, PMF \cite{fusi2018advances} soluciona la tarea de selección automática de flujos de aprendizaje de máquina usando ideas de Filtrado Colaborativo y Optimización Bayesiana. Más específicamente, el problema de seleccionar el pipeline con mejor rendimiento para una tarea en específico es modelado como un problema de filtrado colaborativo que es solucionado usando técnicas de factorización de matrices. Usan el modelo de Factorización de Matrices para ``transferir conocimiento'' de los cientos de diferentes datasets que usan para su entrenamiento. Además utilizan una función de adquisición (\textit{expected improvement}) y un modelo de regresión (Procesos Gausianos) para guiar la exploración del espacio de los posibles pipelines. PMF entrena cada pipeline de ML en una muestra de cada dataset y entonces las evalua en dicho pipeline. Esto resulta en una matriz que resume el rendimiento de cada pipeline de ML en cada dataset. Una vez habiendo observado estas predicciones, la tarea de predecir el rendimiento de cualquier pipeline en un nuevo dataset puede plantearse como un problema de factorización de matrices.

%\textit{Sobre CofiRank}
%
%Para la calificación colaborativa, \textit{Maximum Margin Matrix Factorization} (MMMF) ha demostrado ser un medio eficaz para estimar la función de calificación. MMMF aprovecha los efectos de colaboración: los patrones de calificación de otros usuarios se utilizan para estimar las calificaciones del usuario actual. Una ventaja clave de este enfoque es que funciona sin extracción de características. La extracción de características es específica del dominio, por ejemplo, los procedimientos desarrollados para películas no se pueden aplicar a libros. Por lo tanto, es difícil encontrar un conjunto de características consistente en aplicaciones con muchos tipos diferentes de elementos, como por ejemplo en Amazon. El algoritmo CofiRank se basa en esta idea de MMMF, pero optimiza las medidas de clasificación en lugar de las medidas de clasificación. [poner link del artículo sobre CofiRank]
%
%Dado que solo se presentarán al usuario los elementos mejor clasificados, es mucho más importante clasificar correctamente los primeros elementos que los últimos. En otras palabras, es más importante predecir qué al usuario le gusta de lo que no le gusta. En términos más técnicos, el valor del error para la estimación no es uniforme entre las calificaciones.


\section{Meta-Learning para AutoML}\label{sec:metalearning-automl}


La principal área de investigación de meta-learning estudiada en este trabajo es la selección de algoritmos, la cual ha recibido una considerable cantidad de investigación. En este caso especial de meta-learning, el aspecto de interés es la relación entre las características de los datos y el rendimiento del algoritmo, con el objetivo final de predecir un algoritmo o un conjunto de algoritmos adecuado para un problema específico. Como motivación está el hecho de que es inviable examinar todas las posibles alternativas de algoritmos en un procedimiento de prueba y error. La aplicación de meta-learning en este campo puede por lo tanto ser útil tanto para proveer una recomendación para un usuario final como de paso preliminar para recomendar algoritmos a soluciones más costosas computacionalmente, como los algoritmos de optimización usados en herramientas de AutoML. 


El desafío en meta-learning para la selección de modelos es aprender de experiencia pasada de una forma sistemática e impulsada por los datos. Primero, es necesario coleccionar los meta-datos que describen las tareas de aprendizaje anteriores y los modelos previamente aprendidos. Ellos comprenden las configuraciones exactas de los algoritmos usados para entrenar los modelos, incluyendo:

\begin{itemize}
	\item Las configuraciones de los hiperparámetros, composiciones de los pipelines y/o arquitecturas de redes neuronales.
	\item Las evaluaciones del modelo resultante, tales como la precisión y el tiempo de entrenamiento.
	\item Los parámetros del modelo aprendidos, tales como los pesos entrenados de una red neuronal.
	\item Propiedades medibles de la tarea en sí, también conocidas como meta-features.
\end{itemize}

Luego necesitamos aprender de estos meta-datos previos, para extraer y transferir conocimiento de la búsqueda de los modelos óptimos para nuevas tareas. El resto de esta sección presenta una visión general de diferentes enfoques de meta-learning para hacer esto efectivamente. Además, se muestran ejemplos de cpmo estos enfoques han sido utilizados como paso preliminar en varias herramientas de AutoML.

En la sección las técnicas de meta-learning son separadas en grupos de acuerdo al tipo de meta-datos que ellas aprovechan. Primero, se discute cómo caracterizar las tareas para expresar más explícitamente la similitud entre ellas y cómo construir meta-modelos para aprender las relaciones entre las características de los datos y el rendimiento de las distintas evaluaciones de los modelos [\ref{subsec:mtl_automl_proprerties}], y luego se describe como se aprende solamente de evaluaciones de los modelos [\ref{subsec:mtl_automl_evaluations}].

\subsection{Aprendiendo de las propiedades de las tareas}\label{subsec:mtl_automl_proprerties}


En esta sección se describen un grupo de técnicas de meta-learning que están basadas en las propiedades de las tareas usando meta-features (meta-características) que caracterizan un dataset particular. En este contexto, el problema de meta-learning puede estar definido de la siguiente manera:

\begin{definition}
Consideremos que tenemos acceso a:

\begin{itemize}
	\item Tareas anteriores $t_j \in T$, siendo esto último el conjunto de todas las tareas conocidas.
	\item Un conjunto de algoritmos de aprendizaje, completamente definidos por sus configuraciones $\theta_i \in \Theta$; aquí $\Theta$ representa un espacio de configuración discreto, continuo o mixto que puede cubrir configuraciones de hiperparámetros, componentes de pipeline y/o componentes de una arquitectura de redes.
	\item $P$ el conjunto de todas las puntuaciones de las evaluaciones anteriores $P_{i,j} = P(\theta_i, t_j)$ de una configuración $\theta_i$ en una tarea $t_j$, de acuerdo a una medida de evaluación predefinida, por ejemplo precisión, y una técnica de evaluación de modelos, por ejemplo validación cruzada. 
	\item Un vector $m(t_j) = (m_{j,1}, …, m_{j,K})$ de $K$ caracterizaciones (meta-features) que describe cada tarea $t_j \in T$. $m_{j,k} \in M$ es el conjunto de todas las meta-features conocidas.
%	\item $P_{new}$, el conjunto de todas las evaluaciones conocidas $P_{i, new}$ en una nueva tarea $t_{new}$ 
\end{itemize}

Con esta técnica de meta-learning se puede entrenar un \emph{meta-learner} $L$ que predice el rendimiento de las configuraciones recomendadas $\Theta^*_{new}$ en una nueva tarea $t_{new}$. El meta-learner es entrenado con los meta-datos $P\cup M$, donde $P$ y $M$ son usualmente calculados de antemano, o extraídos de repositorios de meta-datos. 

\end{definition}

La principal característica de esta técnica es el uso de meta-features para medir la similitud de las tareas. Así, por ejemplo, podemos usar la distancia euclidiana entre $m(t_{new})$ y $m(t_j), \forall t_j \in T$ para transferir información de las tareas más similares a la nueva tarea $t_{new}$


\subsubsection{Meta-Features}

\begin{itemize}
	\item[$\checkmark$] Poner algunos ejemplos de meta-features dividido por el tipo de meta-feature qué es.
	\item Hablar de varios papers que definen los distintos tipos de meta-features, puedo además mencionar varios survey de meta-features.
	\item \textit{Meta-Learning and the Full Model Selection Problem}
	\item \textit{Pairwise meta rules for better meta learning based algorithm ranking}
	\item \textit{Selecting Classification Algorithms with Active Testing}
	\item \textit{Towards Reproducible Empirical Research in Meta-Learning}
\end{itemize}

Cómo extraer información adecuada para caracterizar tareas específicas es una de las preguntas fundamentales en meta-learning. Investigadores han intentado contestar esta pregunta observando las características de los datasets que afectan el rendimiento de los algoritmos. Estas caracterizaciones son denominadas meta-features y usualmente se encuentran divididos en cinco grupos. Estos grupos son subconjuntos de medidas de caracterización [poner link de Brazdil] que comparten similitudes entre ellos:


\begin{description}
	\item[Simple:] son características que son fácilmente extraídas de los datos, son conocidas comúnmente y no requieren recursos computacionales significativos. Representan información básica sobre el dataset. Hasta un determinado punto son concebidas para medir la complejidad del problema subyacente. Algunas de las caracterizaciones incluidas en este grupo son: el número de instancias, el número de atributos, la dimensionalidad del dataset, el radio de valores faltantes, etc. También son llamadas medidas \textit{generales}.
	
	\item[Estadísticas:] son características que capturan las propiedades estadísticas de los datos. Estas métricas capturan los indicadores de distribución de datos, tales como la media, la desviación estándar, la correlación y curtosis. Solo caracterizan los atributos numéricos. Las caracterizaciones estadísticas son deterministas y algunas de ellas requieren la definición de valores de hiperparámetros. %, mientras otras pueden generar excepciones, por ejemplo, de división por cero.
	
	\item[Teóricas de la información:] son características del campo de teoría de la información. Estas medidas están basadas en la entropía, la cual captura la cantidad de información en los datos y su complejidad. Ellos pueden ser usados para caracterizar los atributos discretos. Además, son computadas directamente, libres de hiperparámetros, determinísticas y robustas. Semánticamente, describen la variedad y la redundancia de los atributos usados para representar las clases.
	
	\item[Basados en modelos:] son características extraídas de un modelo inducido de los datos de entrenamiento. Las características en este grupo están caracterizadas por la extracción de información de un modelo de aprendizaje de predicción, generalmente, un árbol de decisión. Las medidas caracterizan la complejidad de los problemas basados en las hojas, los nodos y la forma del árbol. Están diseñadas para caracterizar problemas supervisados, todas las medidas son determinísticas, robustas y requieren la definición de los hiperparámetros que hay en el algoritmo de árboles de decisión para inducir el modelo.
	
	\item[\textit{Landmarking}:] son características que usan el rendimiento de algoritmos de aprendizaje simples y rápidos para caracterizar los datasets. Los algoritmos deben tener diferentes sesgos y capturar información importante con un costo computacional bajo. Las medidas caracterizan problemas supervisados y son indirectamente extraídas. Requieren la definición de hiperparámetros: el algoritmo de aprendizaje, la medida de evaluación usada para comprobar el rendimiento del modelo y el procedimiento usado para calcularla (por ejemplo, validación cruzada).
\end{description}

Los primeros tres grupos representan los enfoques más comunes y tradicionales de las caracterizaciones de los datos. Los últimos dos requieren el uso de algoritmos de aprendizaje de máquinas, porque extraen la complejidad del modelo o medidas de rendimiento del mismo, haciendolos además más complejos.

Además de estas meta-features de propósito general, otros conjuntos de meta-features han sido desarrollados. Un ejemplo de esto son los \textit{Pairwise Meta-Rules} (PMR) \cite{sun2013pairwise}, un método de generación de meta-features basados en reglas que comparan el rendimiento de algoritmos individuales en un mismo dataset, formando pares y seleccionando el mejor. La principal motivación detrás de PMR viene de la observación de que conjuntos existentes de meta-features han ignorado el potencial de la relación entre cada par de algoritmos a seleccionar. Añadiendo explícitamente esta información al espacio de meta-features puede mejorar el rendimiento de la predicción de un meta-learner. Sin embargo, para un nuevo dataset esta información no está disponible, por lo que se propone usar un nuevo \textit{rule learner} con el objetivo de aprender reglas para predecir si un algoritmo $\theta_a$ tendrá mejor rendimiento que otro algoritmo $\theta_b$ en un nuevo dataset.

Además de definir manualmente los meta-features, uno puede aprender una representación conjunta para un grupo de tareas. 

Un ejemplo de solución a la generación automática de meta-features es descomponer los meta-features en tres componentes: \textit{meta-function} (\texttt{meta-función}), \textit{object} (\texttt{objeto}) y \textit{post-processing} (\texttt{post-procesamiento}) \cite{Pinto2016TowardsAG, pinto2014framework}. Una \texttt{meta-función} (por ejemplo, entropy) es aplicado a un \texttt{objeto} (por ejemplo, un conjunto de variables independientes) y el resultado es \texttt{post-procesado} (por ejemplo, el promedio), resultando en un meta-feature (por ejemplo, el promedio de la entropía de un atributo) %La generación automática de meta-features está motivada por la selección de una \textit{meta-function} usada para generar sistemáticamente meta-features de todas las posibles combinaciones de alternativas de \texttt{objetos} y \texttt{post-procesamiento} 

\textit{Exploratory Factor Analysis} (EFA)(explicar) es otro método que ha sido utilizado para estudiar el poder predictivo de diferentes \textit{meta-features} usadas en OpenML \cite{bilalli2017predictive}. OpenML es una plataforma colaborativa de aprendizaje automático que está diseñada para guardar y organizar meta-datos sobre datasets, algoritmos de minería de datos, modelos y sus evaluaciones. EFA, más específicamente \textit{Principal Component Analysis} (PCA),  es usado para extraer las características latentes, los cuales son conceptos abstractos que agrupan meta-features junto con características comunes. Después, estudian y visualizan la relación de las carcterísticas latentes con 3 diferentes medidas de rendimiento de 4 algoritmos de clasificación en cientos de datasets disponibles en OpenML, y se seleccionan las características con el mayor poder predictivo. Estas características latentes son usadas para realizar meta-learning y se muestra que su método mejora el proceso de meta-learning.


Varios \textit{frameworks} han sido desarrollados para la selección y extracción automática de metafeatures. Un ejemplo de esto es MFE (\textit{Meta-Feature Extractor})~\cite{Rivolli2018TowardsRE}, una biblioteca de R usada para extraer meta-features de los datasets e identificar problemas más sútiles en su reproducibilidad en la literatura. Propone \textit{guidelines} para la caracterización de datos que fortalece la investigación empírica reproducible en meta-learning. 


%\subsubsection{Selección Automática de Meta-Features}
%
%\begin{itemize}
%	\item[$\checkmark$] Introducir la idea de aprender una representación automáticamente de un grupo de tareas
%	\item Ejemplos
%	\item \textit{Automatic classifier selection for non-experts}
%	\item \textit{Towards Automatic Generation of Meta-Features}
%	\item \textit{On the predictive power of meta-features in OpenML}
%	\item \textit{Towards Reproducible Empirical Research in Meta-Learning}
%	\item \textit{A Framework to decompose and develop metafeatures}
%\end{itemize}

\subsubsection{Meta-modelos}

\begin{itemize}
	\item[$\checkmark$] Explicar en qué consiste
\end{itemize}

Construyendo un meta-modelo o \textit{meta-learner} $L$ podemos aprender relaciones complejas entre los meta-features de una tarea y la utilidad de una configuración específica. Dados los meta-features $M$ de una nueva tarea $t_{new}$ este \textit{meta-learner} $L$ tiene el objetivo de recomendar la configuración más útil $\Theta_{new}^*$ para esta tarea. Existe un gran grupo de trabajos previos construyendo modelos para la selección de algoritmos y recomendación de hiperparámetros. En esta sección se usan ejemplos de varios meta-modelos de acuerdo al tipo de tarea que resuelven: pueden ser usados para rankear un conjunto determinado de configuraciones o para predecir el rendimiento de una nueva tarea basado en una configuración particular. %, formando \textit{clusters} de tareas similares 

\quad

\textbf{Ranking}


\begin{itemize}
	\item[$\checkmark$] Explicar en qué consiste
	\item Poner ejemplos de papers
	\begin{itemize}
		\item \textit{KNN} 	
		\begin{itemize}
			\item \textit{Selection of Time Series Forecasting Models based on Performance Information}
			\item \textit{Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results. (its somewhat old)}
			\item \textit{Approximate Ranking Tree Forests (ART Forests)}
			\item \textit{Pairwise meta rules for better meta learning based algorithm ranking}	
		\end{itemize}
		\item \textit{Clustering Trees:}
		\begin{itemize}
			\item \textit{Ranking with Predictive Clustering Trees}
		\end{itemize}
		\item \textit{XGBoost}
		\begin{itemize}
			\item \textit{RankML: Meta Learning-Based Approach for Pre-Ranking Machine Learning Pipelines}
		\end{itemize}
	\end{itemize}
	\item AutoML (todos siguen un enfoque de \textit{nearest neighbors}) \begin{itemize}
		\item Auto-Sklearn
		\item Alpine Meadow
		\item MOSAIC
		\item ATOMIC
		\item SmartML
	\end{itemize}
\end{itemize}

\quad


Los meta-modelos pueden ser usados para, dado un conjunto de meta-features $M$ y una nueva tarea $t_{new}$ generar un \textit{ranking} de las mejores $K$ configuraciones. Se obtiene así un conjunto prometedor de modelos con sus hiperparámetros para esta nueva tarea.
 
Uno de los enfoques más populares es construir un meta-modelo de \textit{K-Nearest Neighbor} (kNN) para predecir las tareas que son similares, y luego rankear las mejores configuraciones en estas tareas similares \cite{santos2004selection, bradzil2003ranking}. En estos trabajos se usan una evaluación multicriterio que toma en cuenta la precisión obtenida y el tiempo de entrenamiento de una configuración: \textit{Adjusted Ratio of Ratios} (ARR) o el Radio Ajustado de Radios (explicar) para formar los rankings. Existen muchas medidas para la evaluación de un ranking determinado, el más utilizado es el coeficiente de correlación de Spearman, el cual mide la distancia del ranking recomendando al ranking ideal. El ranking ideal corresponde al ordenamiento correcto de los modelos candidatos para una tarea determinada.
 
Meta-modelos especificamente destinados para ranking, como \textit{Predictive Clustering Trees} (PCTs) también han sido usados \cite{todorovski2002ranking}. Los PCTs son una generalización bien establecida de los árboles de decisión estándares, los cuales pueden ser usados para solucionar una variedad de tareas de predicción de modelos \cite{tomaz2020oblique}, estos permiten la predicción de múltiples variables de destino. Un solo predicting clustering tree tiene la habilidad de predecir el rendimiento de todos los algoritmos de aprendizaje a la vez. Por lo tanto, además de obtener un ranking, obtenemos una explicación para el mismo. Al igual que en otros trabajos, el coeficiente de correlación de Spearman es usado para medir el error entre los rankings.
 
Los \textit{Aproximate Ranking Trees Forest} (\textit{ART Forest}) \cite{sun2013pairwise} son \textit{ensembles} de árboles de ranking rápidos que han probado ser especialmente efectivos, ya que ellos tienen `incorporados' la selección de meta-features, funcionan bien incluso si hay pocas tareas disponibles y el \textit{ensembling} hace el método más robusto.


\quad

\textbf{Clustering}

\begin{itemize}
	\item AutoML \begin{itemize}
		\item AutoSSL
	\end{itemize}
\end{itemize}

\quad

\textbf{Predicción de Rendimiento}

\begin{itemize}
	\item[$\checkmark$] Explicar en qué consiste
	\item Poner ejemplos de papers
	\item \textit{Predicting the Performance of Learning Algorithms Using Support Vector Machines as Meta-regressors}
	\item \textit{Selection Classification Algorithms with Active Testing}
	\item \textit{Automatic classifier selection for non-experts}
\end{itemize}

\quad

Además, los meta-modelos pueden predecir directamente el rendimiento, es decir, la predicción o el tiempo de entrenamiento de una configuración en una tarea dada, dada sus meta-features. Esto nos permite estimar qué tan bueno o malo una configuración va a ser, y si vale la pena evaluarla mediante cualquier procedimiento de optimización en una nueva tarea.

Uno de los principales enfoques para construir modelos que predicen el rendimiento es el uso de \textit{meta-regresors} o meta-regresores. \textit{Meta-Regression} es un enfoque específico de meta-learning que consiste en el uso de un algoritmo de regresión para predecir el valor de una medida de rendimiento específica (por ejemplo, precisión) de los algoritmos candidatos basándose en las características del problema. 

El objetivo de usar regresión en meta-learning es predecir el rendimiento real de cada clasificador idenpendientemente en vez de predecir el mejor de un par o de todos los algoritmos. Por lo tanto, un modelo separado de regresión es entrenado para cada algoritmo. El conocimiento del meta-modelo es derivado de los datos de entrenamiento, el cual está compuesto por las meta-features de múltiples datasets y una variable a predecir. Los rendimientos reales de los algoritmos sirven como una variable destino cuantitativa en este contexto. Cada dataset resulta en una instancia en los datos de entrenamiento descritos por sus meta-features y el rendimiento calculado del algortimo destino. Usando estas medidas, un modelo de regresión es aprendido que describe las relaciones entre los meta-features y el rendimiento esperado de los datasets.

Si el rendimiento de un algoritmo necesita ser predecido para un nuevo dataset, entonces los meta-features de este dataset son computados primero. Entonces, el modelo de regresión aprendido con anterioridad es aplicado para esos valores de características. El resultado final entonces es el rendimiento del clasificador para este nuevo dataset.

Una las principales desventajas de este enfoque de regresión, es que para cada algoritmo destino es entrenado un modelo diferente de regresión. Esto puede consumir gran cantidad de tiempo, especialmente cuando una optimización de hiperparámetros exhaustiva para los meta-modelos es realizada. Sin embargo, modelos separados simplifican el proceso de añadir y eliminar algoritmos destinos. Además, esto permite que se fácil el entrenamiento de diferentes variantes de un mismo algoritmo. 

Los SVM han sido entrenados como meta-regresores de algoritmos de clasificación para predecir su precisión. En una versión de esto se usan las configuraciones por defecto de los algoritmos, y para una nueva tarea $t_{new}$ dada sus meta-features se predice el resultado \cite{guerra2008predicting}. En este trabajo los experimentos fueron llevados a cabo prediciendo el rendimiento de distintas redes de \textit{Multi-Layer Perceptron} (MLP). Los SVMs fueron aplicados con diferentes funciones kernel para predecir el valor del \textit{Normalized Mean of Squared Errors} (NMSE) o Error Cuadrático Medio Normalizado de los MLPs

Posteriormente, se entrenó un meta-regresor similar \cite{reif2012automatic} para predecir el rendimiento \textit{optimizado} de un mayor número de algoritmos de clasificación. En este trabajo el parámetro más importante de cada algoritmo fue optimizado en orden de obtener un rendimiento más realista y por lo tanto una mejor evaluación. Para una evaluación más exhaustiva, el \textit{root mean squared error} (RMSE) y el coeficiente de correlación de Pearson \textit{Pearson product-moment correlation coeficient} (PMCC) fue usado como medida de rendimiento. RMSE tiene la ventaja de proveer un intervalo de confianza junto con la predicción, mientras PMCC también puede medir derivaciones relativas Además se realiza una selección automática de características que es aplicada dos veces de acuerdo a ambas las medidas de rendimiento e independientemente para cada clasificador. Los resultados son comparados para cada de los distintos grupos de meta-features así como el conjunto completo de meta-features.

\quad

\textbf{Sintesis de \textit{Pipelines}}

\begin{itemize}
	\item[$\checkmark$] Explicar en qué consiste
	\item Poner ejemplos de papers
	\item \textit{Towards Automatic Machine Learning Pipeline Design}
	\item \textit{AlphaD3M}
	\item \textit{RankML: Meta Learning-Based Approach for Pre-Ranking Machine Learning Pipelines}
\end{itemize}

\quad

Cuando se crean pipelines enteros de machine learning, el número de opciones de configuración crece dinámicamente, haciendolo incluso más importante para aprovechar la experiencia previa. Uno puede controlar el espacio de búsqueda imponiendo una estructura fija en el pipeline, completamente descrita por sus hiperparámetros. Luego uno puede usar los pipelines más prometedores para inicializar otros procesos de optimización.

\subsubsection{Ajustar o no ajustar?}

\begin{itemize}
	\item[$\checkmark$] Introducir este tema
	\item Poner ejemplos de papers
	\item \textit{Using Metalearning to Predict When Parameter Optimization Is Likely to Improve Classification Accuracy}
	\item \textit{Informing the Use of Hyperparameter Optimization Through Metalearning}
\end{itemize}

Para reducir el número de configuraciones de parámetros a ser optimizados, y para salvar valioso tiempo de optimización en las configuraciones con restricciones de tiempo. Los meta-modelos también han sido propuestos para predecir si un algoritmo dado vale la pena ser ajustado dados las meta-features de la tarea en cuestión y cuánta mejora puede ser esperada del ajuste de un algoritmo específico contra la inversión de tiempo adicional.

%\subsubsection{Otras técnicas}
%
%\begin{itemize}
%	\item Introducir otras técnicas y poner ejemplos de papers
%	\item Collaborative Filtering
%	\item \textit{Recommending Learning Algorithms and Their Associated Hyperparameters}
%	\item Los ejemplos de sistemas de AutoML que usan CF
%\end{itemize}

\subsection{Aprendiendo de las evaluaciones de modelos}\label{subsec:mtl_automl_evaluations}

\begin{itemize}
	\item Falta un poco de introducción...
	\item[$\checkmark$] Explicar en qué consiste
\end{itemize}

Otro grupo de técnicas de meta-learning están basadas 	en aprender de evaluaciones de los modelos. En este contexto, el problema está definido de la siguiente manera:

\begin{definition}
Consideremos que tenemos acceso a:

\begin{itemize}
	\item Tareas anteriores $t_j \in T$.
	\item Un conjunto de algoritmos de aprendizaje, completamente definidos por sus configuraciones $\theta_i \in \Theta$.
	\item $P$ el conjunto de todas las evaluaciones escalares anteriores $P_{i,j} = P(\theta_i, t_j)$ de una configuración $\theta_i$ en una tarea $t_j$, de acuerdo a una medida de evaluación predefinida y una técnica de evaluación de modelos. 
	\item $P_{new}$, el conjunto de todas las evaluaciones conocidas $P_{i, new}$ en una nueva tarea $t_{new}$ 
\end{itemize}

Ahora queremos entrenar un \emph{meta-learner} $L$ que predice las configuraciones recomendadas $\Theta^*_{new}$ para una nueva tarea $t_{new}$. El meta-learner es entrenado en meta-data $P\cup P_{new}$ donde:

\begin{itemize}
	\item $P$ usualmente es recogido de antemano, o extraído de repositorios de meta-data.
	\item $P_{new}$ es aprendido por la técnica de meta-lerning en sí de una forma iterativa, algunas veces inicializado con un $P'_{new}$ inicial generado por otro método.
\end{itemize}
\end{definition}

Estas técnicas son usadas generalmente para recomendar configuraciones y espacios de búsqueda útiles, así como transferir conocimiento de tareas \textit{empíricamente} similares. Algunas de estas técnicas son explicadas a continuación. 

\subsubsection{Recomendaciones independientes de la tarea}

\begin{itemize}
	\item[$\checkmark$] Explicar en qué consiste
	\item Poner ejemplos
	\begin{itemize}
		\item \textit{Selecting Classification Algorithms with Active Testing}
		\item \textit{Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results}
		\item \textit{Speeding up algorithm selection using average ranking and active testing by introducing runtime}
		\item \textit{Learning hyperparameter optimization initializations}
		\item AutoML: \begin{itemize}
			\item Auto-Pytorch
			\item Auto-Sklearn 2.0
		\end{itemize}
	\end{itemize}
\end{itemize}

Supongamos que $P_{new} = 0$, por lo que no tenemos acceso a ninguna evaluación de $t_{new}$. Aún así se puede aprender una función $f: \Theta \times T \rightarrow \{\theta^*_k\}, k=1..K$, resultando en un conjunto de evaluaciones recomendadas \textit{independientes} de $t_{new}$. Estos $\theta^*_k$ pueden ser evaluados en $t_{new}$ para seleccionar el mejor, o para inicializar otros enfoques de optimización.

Tales enfoques a menudo producen un ranking, es decir, un conjunto ordenado $\theta^*_k$. Esto es típicamente hecho discreteando $\theta$ en un conjunto de configuraciones candidatas $\theta_i$, también llamada \textit{portafolio}, evaluado en un gran número de tareas $t_j$. Luego podemos construir un ranking por clase, por ejemplo usando \textit{success rates}, \textit{AUC}, o \textit{significant wins}. % Sin embargo, a menudo es deseable que algortimos igualmente buenos pero más rápidos estén rankeados mejor, y múltiples métodos han sido propuestos para compensar tiempo de entrenamiento y precisión. Luego podemos agregar estos rankings de una sola tarea en rankings globales, por ejemplo calculando el rank promedio a través de todas las tareas. Cuando hay insuficientes datos para construir un ranking global, uno puede recomendar un subconjunto de configuraciones basados en la configuración mejor conocida para cada tarea anterior, o retornar quasi-lineal rankings.

Para encontrar el mejor $\theta^*$ para una tarea $t_{new}$ nunca vista antes, un método simple es seleccionar las $k$ mejores configuraciones, bajando en la lista y evaluando cada configuración en $t_{new}$. Esta evaluación puede ser terminada después de un valor predefinido para $k$, un presupuesto de tiempo, o cuando un modelo lo suficientemente preciso es encontrado. 

\subsubsection{Diseño del espacio de configuración}

\begin{itemize}
	\item[$\checkmark$] Explicar en qué consiste
	\item Poner ejemplos
	\item Functional ANOVA
	\item \textit{Hyperparameter Importance Across Datasets}
	\item \textit{Hyperparameter Search Space Pruning - A New Component for Sequential Model-Based Hyperparameter Optimization}
	\item AutoML \begin{itemize}
		\item ATM
	\end{itemize}
\end{itemize}


Las evaluaciones previas también pueden ser usadas para aprender mejores \textit{espacios de configuración} $\Theta^*$. Incluso siendo independientes de $t_{new}$, esto puede radicalmente acelerar la búsqueda para modelos óptimos, ya que solo las regiones más relevantes de los espacios de configuración son explorados. Esto es crítico cuando los recursos computacionales están limitados. 

\subsubsection{Transferencia de la configuración}

Si se quiere dar recomendaciones para una tarea específica $t_{new}$, necesitamos información sobre que tan similar $t_{new}$ es a las tareas anteriores $t_j$. Una manera de hacer esto es evaluando un número de configuraciones recomendadas (o parcialmente aleatorias) en $t_{new}$, dando lugar a nueva evidencia $P_{new}$. Si luego observamos que las evaluaciones de $P_{i,new}$ son similares a $P_{i,j}$, entonces $t_j$ y $t_{new}$ pueden ser consideradas similares, basadas en evidencia empírica. Se puede incluir este conocimiento para entrenar un meta-learner que predice un conjunto de configuraciones recomendadas $\Theta_{new}^*$ para $t_{new}$. Además, cada $\theta^*_{new}$ seleccionado puede ser evaluado y luego incluido en $P_{new}$, repitiendo el ciclo y coleccionando más evidencia empírica para aprender cuales tareas son similares entre sí.

\quad 

\textbf{Landmarks relativos}

\begin{itemize}
	\item[$\checkmark$] Explicar en qué consiste
	\item Hablar de Active Testing
	\item Poner ejemplos
	\item \textit{Pairwise meta rules for better meta learning based algorithm ranking}
	\item \textit{Toward Automatic Generation of Meta-features}
	\item Automl \begin{itemize}
		\item ActivMetaL
	\end{itemize}
\end{itemize}

\quad

Una primera medida para la similaridad de las tareas considera las diferencias en el rendimiento relativas (pairwise), también llamadas \textit{relative landmarks}, $RL_{a,b,j} = P_{a,j} - P_{b,j}$ entre dos configuraciones $\theta_a$ y $\theta_b$ en una tarea particular $t_j$. (hablar de los papers que tratan de esto) 

\quad

\textbf{Modelos sustitutos}

\begin{itemize}
	\item[$\checkmark$] Explicar en qué consiste
	\item Me falta explicar qué es un modelo sustituto
	\item Poner ejemplos
	\item \textit{Collaborative hyperparameter tuning}
	\item \textit{Efficient Transfer Learning Method for Automatic Hyperparameter Tuning}
	\item \textit{Hyperparameter Optimization with Factorized Multilayer Perceptrons}
	\item \textit{Scalable Gaussian process-based transfer surrogates for hyperparameter optimization.}
	\item AutoML \begin{itemize}
		\item OBOE
		\item PMF
		\item TensorOBOE
		\item Oracle AutoML
	\end{itemize}
\end{itemize}

\quad

Una forma más flexible de transferir información es construir un \textit{modelo sustituto} $s_j(\theta_i) = P_{i,j}$ para todas las tareas anteriores $t_j$, entrenadas usando todo el conjunto $P$ disponible. Uno puede definir la similitud entre tareas en términos de error entre $s_j(\theta)$ y $P_{i, new}$: si el modelo sustituto para $t_j$ puede generar predicciones precisas para $t_{new}$, entonces esas tareas son intrínsicamente similares. Esto es realizado usualmente en combinación con la optimización Bayesiana para determinar el siguiente $\theta_i$. 



\section{Conclusión}\label{sec:conclusion}

Concluir el capítulo


%\\ \\
%Las matrices $\widetilde{M}_{n}$, $L$, $r$ est\'{a}n definidas como en el esquema anterior.\\
%El esquema~(\ref{LLDP scheme}) lo denotaremos LLDP y su principal objetivo es resolver 
%problemas donde la parte stiff es solo la lineal.
