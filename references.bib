@article{baker2016designing,
	title={Designing neural network architectures using reinforcement learning},
	author={Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
	journal={arXiv preprint arXiv:1611.02167},
	year={2016}
}

@inproceedings{bergstra2013hyperopt,
	title={Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms},
	author={Bergstra, James and Yamins, Dan and Cox, David D and others},
	booktitle={Proceedings of the 12th Python in science conference},
	volume={13},
	pages={20},
	year={2013},
	organization={Citeseer}
}

@article{bischl2016aslib,
	title={Aslib: A benchmark library for algorithm selection},
	author={Bischl, Bernd and Kerschke, Pascal and Kotthoff, Lars and Lindauer, Marius and Malitsky, Yuri and Fr{\'e}chette, Alexandre and Hoos, Holger and Hutter, Frank and Leyton-Brown, Kevin and Tierney, Kevin and others},
	journal={Artificial Intelligence},
	volume={237},
	pages={41--58},
	year={2016},
	publisher={Elsevier}
}


@inproceedings{de2017recipe,
	title={RECIPE: a grammar-based framework for automatically evolving classification pipelines},
	author={de S{\'a}, Alex GC and Pinto, Walter Jos{\'e} GS and Oliveira, Luiz Otavio VB and Pappa, Gisele L},
	booktitle={European Conference on Genetic Programming},
	pages={246--261},
	year={2017},
	organization={Springer}
}

@inproceedings{de2018automated,
	title={Automated selection and configuration of multi-label classification algorithms with grammar-based genetic programming},
	author={de S{\'a}, Alex GC and Freitas, Alex A and Pappa, Gisele L},
	booktitle={International Conference on Parallel Problem Solving from Nature},
	pages={308--320},
	year={2018},
	organization={Springer}
}

@article{elshawi2019automated,
	title={Automated machine learning: State-of-the-art and open challenges},
	author={Elshawi, Radwa and Maher, Mohamed and Sakr, Sherif},
	journal={arXiv preprint arXiv:1906.02287},
	year={2019}
}

@inproceedings{jin2019auto,
	title={Auto-keras: An efficient neural architecture search system},
	author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
	booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages={1946--1956},
	year={2019}
}

@inproceedings{komer2014hyperopt,
	title={Hyperopt-sklearn: automatic hyperparameter configuration for scikit-learn},
	author={Komer, Brent and Bergstra, James and Eliasmith, Chris},
	booktitle={ICML workshop on AutoML},
	volume={9},
	pages={50},
	year={2014},
	organization={Citeseer}
}

@inproceedings{maher2019smartml,
	title={Smartml: A meta learning-based framework for automated selection and hyperparameter tuning for machine learning algorithms},
	author={Maher, Mohamed and Sakr, Sherif},
	booktitle={EDBT: 22nd International Conference on Extending Database Technology},
	year={2019}
}

@inproceedings{mendoza2016towards,
	title={Towards automatically-tuned neural networks},
	author={Mendoza, Hector and Klein, Aaron and Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
	booktitle={Workshop on Automatic Machine Learning},
	pages={58--65},
	year={2016},
	organization={PMLR}
}

@article{mohr2018ml,
	title={ML-Plan: Automated machine learning via hierarchical planning},
	author={Mohr, Felix and Wever, Marcel and H{\"u}llermeier, Eyke},
	journal={Machine Learning},
	volume={107},
	number={8},
	pages={1495--1515},
	year={2018},
	publisher={Springer}
}

@inproceedings{fusi2018advances,
	author = {Fusi, Nicolo and Sheth, Rishit and Elibol, Melih},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Probabilistic Matrix Factorization for Automated Machine Learning},
	url = {https://proceedings.neurips.cc/paper/2018/file/b59a51a3c0bf9c5228fde841714f523a-Paper.pdf},
	volume = {31},
	year = {2018}
}


@article{yang2018oboe,
	author    = {Chengrun Yang and
	Yuji Akimoto and
	Dae Won Kim and
	Madeleine Udell},
	title     = {{OBOE:} Collaborative Filtering for AutoML Initialization},
	journal   = {CoRR},
	volume    = {abs/1808.03233},
	year      = {2018},
	url       = {http://arxiv.org/abs/1808.03233},
	archivePrefix = {arXiv},
	eprint    = {1808.03233},
	timestamp = {Sun, 02 Sep 2018 15:01:57 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1808-03233.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{olson2016evaluation,
	title={Evaluation of a tree-based pipeline optimization tool for automating data science},
	author={Olson, Randal S and Bartley, Nathan and Urbanowicz, Ryan J and Moore, Jason H},
	booktitle={Proceedings of the genetic and evolutionary computation conference 2016},
	pages={485--492},
	year={2016}
}

@inproceedings{pham2018efficient,
	title={Efficient neural architecture search via parameters sharing},
	author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
	booktitle={International Conference on Machine Learning},
	pages={4095--4104},
	year={2018},
	organization={PMLR}
}

@article{rakotoarison2019automated,
	title={Automated machine learning with Monte-Carlo Tree Search},
	author={Rakotoarison, Herilalaina and Schoenauer, Marc and Sebag, Mich{\`e}le},
	journal={arXiv preprint arXiv:1906.00170},
	year={2019}
}

@inproceedings{evans2020adaptive,
	title={An adaptive and near parameter-free evolutionary computation approach towards true automation in automl},
	author={Evans, Benjamin and Xue, Bing and Zhang, Mengjie},
	booktitle={2020 IEEE Congress on Evolutionary Computation (CEC)},
	pages={1--8},
	year={2020},
	organization={IEEE}
}

@inproceedings{shang2019democratizing,
	title={Democratizing data science through interactive curation of ml pipelines},
	author={Shang, Zeyuan and Zgraggen, Emanuel and Buratti, Benedetto and Kossmann, Ferdinand and Eichmann, Philipp and Chung, Yeounoh and Binnig, Carsten and Upfal, Eli and Kraska, Tim},
	booktitle={Proceedings of the 2019 International Conference on Management of Data},
	pages={1171--1188},
	year={2019}
}

@inproceedings{thornton2013auto,
	title={Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms},
	author={Thornton, Chris and Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
	booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages={847--855},
	year={2013}
}

@article{wang2018rafiki,
	title={Rafiki: Machine learning as an analytics service system},
	author={Wang, Wei and Wang, Sheng and Gao, Jinyang and Zhang, Meihui and Chen, Gang and Ng, Teck Khim and Ooi, Beng Chin},
	journal={arXiv preprint arXiv:1804.06087},
	year={2018}
}


@article{wang2021flaml,
	title={FLAML: A Fast and Lightweight AutoML Library},
	author={Wang, Chi and Wu, Qingyun and Weimer, Markus and Zhu, Erkang},
	journal={Proceedings of Machine Learning and Systems},
	volume={3},
	year={2021}
}

@article{zoph2016neural,
	title={Neural architecture search with reinforcement learning},
	author={Zoph, Barret and Le, Quoc V},
	journal={arXiv preprint arXiv:1611.01578},
	year={2016}
}

@inproceedings{swearingen2017atm,
	title={ATM: A distributed, collaborative, scalable system for automated machine learning},
	author={Swearingen, Thomas and Drevo, Will and Cyphers, Bennett and Cuesta-Infante, Alfredo and Ross, Arun and Veeramachaneni, Kalyan},
	booktitle={2017 IEEE International Conference on Big Data (Big Data)},
	pages={151--162},
	year={2017},
	organization={IEEE}
}

@inproceedings{yang2020automl,
	title={Automl pipeline selection: Efficiently navigating the combinatorial space},
	author={Yang, Chengrun and Fan, Jicong and Wu, Ziyang and Udell, Madeleine},
	booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages={1446--1456},
	year={2020}
}

@article{zimmer2021auto,
	title={Auto-Pytorch: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL},
	author={Zimmer, Lucas and Lindauer, Marius and Hutter, Frank},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	year={2021},
	publisher={IEEE}
}

@inproceedings{drori2018alphad3m,
	title={AlphaD3M: Machine learning pipeline synthesis},
	author={Drori, Iddo and Krishnamurthy, Yamuna and Rampin, Remi and Louren{\c{c}}o, Raoni and One, Jorge and Cho, Kyunghyun and Silva, Claudio and Freire, Juliana},
	booktitle={AutoML Workshop at ICML},
	year={2018}
}

@inproceedings{fuerer2015efficient,
	author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Efficient and Robust Automated Machine Learning},
	url = {https://proceedings.neurips.cc/paper/2015/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf},
	volume = {28},
	year = {2015}
}

@inbook{olson2019tpot,
	author="Olson, Randal S.
	and Moore, Jason H.",
	title="TPOT: A Tree-Based Pipeline Optimization Tool for Automating Machine Learning",
	bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
	year="2019",
	publisher="Springer International Publishing",
	address="Cham",
	pages="151--160",
	abstract="As data science becomes increasingly mainstream, there will be an ever-growing demand for data science tools that are more accessible, flexible, and scalable. In response to this demand, automated machine learning (AutoML) researchers have begun building systems that automate the process of designing and optimizing machine learning pipelines. In this chapter we present TPOT v0.3, an open source genetic programming-based AutoML system that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification task. We benchmark TPOT on a series of 150 supervised classification tasks and find that it significantly outperforms a basic machine learning analysis in 21 of them, while experiencing minimal degradation in accuracy on 4 of the benchmarks---all without any domain knowledge nor human input. As such, genetic programming-based AutoML systems show considerable promise in the AutoML domain.",
	isbn="978-3-030-05318-5",
	doi="10.1007/978-3-030-05318-5_8",
	url="https://doi.org/10.1007/978-3-030-05318-5_8"
}

@inproceedings{chen2018autostacker,
	author = {Chen, Boyuan and Wu, Harvey and Mo, Warren and Chattopadhyay, Ishanu and Lipson, Hod},
	title = {Autostacker: A Compositional Evolutionary Learning System},
	year = {2018},
	isbn = {9781450356183},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3205455.3205586},
	doi = {10.1145/3205455.3205586},
	abstract = {In this work, an automatic machine learning (AutoML) modeling architecture called
	Autostacker is introduced. Autostacker combines an innovative hierarchical stacking
	architecture and an evolutionary algorithm (EA) to perform efficient parameter search
	without the need for prior domain knowledge about the data or feature preprocessing.
	Using EA, Autostacker quickly evolves candidate pipelines with high predictive accuracy.
	These pipelines can be used in their given form, or serve as a starting point for
	further augmentation and refinement by human experts. Autostacker finds innovative
	machine learning model combinations and structures, rather than selecting a single
	model and optimizing its hyperparameters. When its performance on fifteen datasets
	is compared with that of other AutoML systems, Autostacker produces superior or competitive
	results in terms of both test accuracy and time cost.},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	pages = {402–409},
	numpages = {8},
	keywords = {autoML, evolutionary machine learning, machine learning},
	location = {Kyoto, Japan},
	series = {GECCO '18}
}

@inproceedings{real2020automl,
	title={Automl-zero: Evolving machine learning algorithms from scratch},
	author={Real, Esteban and Liang, Chen and So, David and Le, Quoc},
	booktitle={International Conference on Machine Learning},
	pages={8007--8019},
	year={2020},
	organization={PMLR}
}

@article{NSGA-II,
	author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
	title = {A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II},
	year = {2002},
	issue_date = {April 2002},
	publisher = {IEEE Press},
	volume = {6},
	number = {2},
	issn = {1089-778X},
	url = {https://doi.org/10.1109/4235.996017},
	doi = {10.1109/4235.996017},
	abstract = {Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and
	sharing have been criticized mainly for: (1) their O(MN3) computational complexity
	(where M is the number of objectives and N is the population size); (2) their non-elitism
	approach; and (3) the need to specify a sharing parameter. In this paper, we suggest
	a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic
	Algorithm II), which alleviates all of the above three difficulties. Specifically,
	a fast non-dominated sorting approach with O(MN2) computational complexity is presented.
	Also, a selection operator is presented that creates a mating pool by combining the
	parent and offspring populations and selecting the best N solutions (with respect
	to fitness and spread). Simulation results on difficult test problems show that NSGA-II
	is able, for most problems, to find a much better spread of solutions and better convergence
	near the true Pareto-optimal front compared to the Pareto-archived evolution strategy
	and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay
	special attention to creating a diverse Pareto-optimal front. Moreover, we modify
	the definition of dominance in order to solve constrained multi-objective problems
	efficiently. Simulation results of the constrained NSGA-II on a number of test problems,
	including a five-objective, seven-constraint nonlinear problem, are compared with
	another constrained multi-objective optimizer, and the much better performance of
	NSGA-II is observed},
	journal = {Trans. Evol. Comp},
	month = apr,
	pages = {182–197},
	numpages = {16}
}

@article{alphazero,
	author    = {David Silver and
	Thomas Hubert and
	Julian Schrittwieser and
	Ioannis Antonoglou and
	Matthew Lai and
	Arthur Guez and
	Marc Lanctot and
	Laurent Sifre and
	Dharshan Kumaran and
	Thore Graepel and
	Timothy P. Lillicrap and
	Karen Simonyan and
	Demis Hassabis},
	title     = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
	Learning Algorithm},
	journal   = {CoRR},
	volume    = {abs/1712.01815},
	year      = {2017},
	url       = {http://arxiv.org/abs/1712.01815},
	eprinttype = {arXiv},
	eprint    = {1712.01815},
	timestamp = {Mon, 13 Aug 2018 16:46:01 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{OracleAutoML,
	author = {Yakovlev, Anatoly and Moghadam, Hesam Fathi and Moharrer, Ali and Cai, Jingxiao and Chavoshi, Nikan and Varadarajan, Venkatanathan and Agrawal, Sandeep R. and Idicula, Sam and Karnagel, Tomas and Jinturkar, Sanjay and Agarwal, Nipun},
	title = {Oracle AutoML: A Fast and Predictive AutoML Pipeline},
	year = {2020},
	issue_date = {August 2020},
	publisher = {VLDB Endowment},
	volume = {13},
	number = {12},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3415478.3415542},
	doi = {10.14778/3415478.3415542},
	abstract = {Machine learning (ML) is at the forefront of the rising popularity of data-driven
	software applications. The resulting rapid proliferation of ML technology, explosive
	data growth, and shortage of data science expertise have caused the industry to face
	increasingly challenging demands to keep up with fast-paced develop-and-deploy model
	lifecycles. Recent academic and industrial research efforts have started to address
	this problem through automated machine learning (AutoML) pipelines and have focused
	on model performance as the first-order design objective. We present Oracle AutoML,
	a novel iteration-free AutoML pipeline designed to not only provide accurate models,
	but also in a shorter runtime. We are able to achieve these objectives by eliminating
	the need to continuously iterate over various pipeline configurations. In our feed-forward
	approach, each pipeline stage makes decisions based on metalearned proxy models that
	can predict candidate pipeline configuration performances before building the full
	final model. Our approach, which builds and tunes only the best candidate pipeline,
	achieves better scores at a fraction of the time compared to state-of-the-art open
	source AutoML tools, such as H2O and Auto-sklearn. This makes Oracle AutoML a prime
	candidate for addressing current industry challenges.},
	journal = {Proc. VLDB Endow.},
	month = aug,
	pages = {3166–3180},
	numpages = {15}
}

@article{sun2013pairwise,
	title={Pairwise meta-rules for better meta-learning-based algorithm ranking},
	author={Sun, Quan and Pfahringer, Bernhard},
	journal={Machine learning},
	volume={93},
	number={1},
	pages={141--161},
	year={2013},
	publisher={Springer}
}

@inproceedings{Pinto2016TowardsAG,
	title={Towards Automatic Generation of Metafeatures},
	author={F{\'a}bio Pinto and C. Soares and Jo{\~a}o Mendes-Moreira},
	booktitle={PAKDD},
	year={2016}
}

@inproceedings{pinto2014framework,
	author = {Pinto, F\'{a}bio and Soares, Carlos and Mendes-Moreira, Jo\~{a}o},
	title = {A Framework to Decompose and Develop Metafeatures},
	year = {2014},
	isbn = {16130073},
	publisher = {CEUR-WS.org},
	address = {Aachen, DEU},
	abstract = {This paper proposes a framework to decompose and develop metafeatures for Metalearning
	(MtL) problems. Several metafeatures (also known as data characteristics) are proposed
	in the literature for a wide range of problems. Since MtL applicability is very general
	but problem dependent, researchers focus on generating specific and yet informative
	metafeatures for each problem. This process is carried without any sort of conceptual
	framework. We believe that such framework would open new horizons on the development
	of metafeatures and also aid the process of understanding the metafeatures already
	proposed in the state-of-the-art. We propose a framework with the aim of fill that
	gap and we show its applicability in a scenario of algorithm recommendation for regression
	problems.},
	booktitle = {Proceedings of the 2014 International Conference on Meta-Learning and Algorithm Selection - Volume 1201},
	pages = {32–36},
	numpages = {5},
	location = {Prague, Czech Republic},
	series = {MLAS'14}
}

@article{bilalli2017predictive,
	author = {Bilalli, Besim and Abell\'{o}, Alberto and Aluja-Banet, Tom\'{\i}s},
	title = {On the Predictive Power of Meta-Features in OpenML},
	year = {2017},
	issue_date = {20 12 2017},
	publisher = {Walter de Gruyter &amp; Co.},
	address = {USA},
	volume = {27},
	number = {4},
	issn = {1641-876X},
	url = {https://doi.org/10.1515/amcs-2017-0048},
	doi = {10.1515/amcs-2017-0048},
	abstract = {Abstract The demand for performing data analysis is steadily rising. As a consequence,
	people of different profiles i.e., nonexperienced users have started to analyze their
	data. However, this is challenging for them. A key step that poses difficulties and
	determines the success of the analysis is data mining model/algorithm selection problem.
	Meta-learning is a technique used for assisting non-expert users in this step. The
	effectiveness of meta-learning is, however, largely dependent on the description/characterization
	of datasets i.e., meta-features used for meta-learning. There is a need for improving
	the effectiveness of meta-learning by identifying and designing more predictive meta-features.
	In this work, we use a method from exploratory factor analysis to study the predictive
	power of different meta-features collected in OpenML, which is a collaborative machine
	learning platform that is designed to store and organize meta-data about datasets,
	data mining algorithms, models and their evaluations. We first use the method to extract
	latent features, which are abstract concepts that group together meta-features with
	common characteristics. Then, we study and visualize the relationship of the latent
	features with three different performance measures of four classification algorithms
	on hundreds of datasets available in OpenML, and we select the latent features with
	the highest predictive power. Finally, we use the selected latent features to perform
	meta-learning and we show that our method improves the meta-learning process. Furthermore,
	we design an easy to use application for retrieving different meta-data from OpenML
	as the biggest source of data in this domain.},
	journal = {Int. J. Appl. Math. Comput. Sci.},
	month = dec,
	pages = {697–712},
	numpages = {16},
	keywords = {feature extraction, meta-learning, feature selection}
}

@article{Rivolli2018TowardsRE,
	title={Towards Reproducible Empirical Research in Meta-Learning},
	author={Adriano Rivolli and L. P. F. Garcia and Carlos Soares and J. Vanschoren and A. Carvalho},
	journal={ArXiv},
	year={2018},
	volume={abs/1808.10406}
}

@inproceedings{santos2004selection,
	author = {Santos, Patrícia and Ludermir, Teresa and Prudêncio, Ricardo},
	year = {2004},
	month = {01},
	pages = {366-371},
	title = {Selection of Time Series Forecasting Models based on Performance Information},
	journal = {Proceedings - HIS'04: 4th International Conference on Hybrid Intelligent Systems},
	doi = {10.1109/ICHIS.2004.86}
}

@article{bradzil2003ranking,
	author = {Brazdil, Pavel and Soares, Carlos and Costa, Joaquim},
	year = {2003},
	month = {03},
	pages = {251-277},
	title = {Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results},
	volume = {50},
	journal = {Machine Learning},
	doi = {10.1023/A:1021713901879}
}

@inproceedings{todorovski2002ranking,
	author = {Todorovski, Ljupco and Blockeel, Hendrik and Džeroski, Sašo},
	year = {2002},
	month = {08},
	pages = {444-455},
	title = {Ranking with Predictive Clustering Trees},
	volume = {2430},
	isbn = {978-3-540-44036-9},
	doi = {10.1007/3-540-36755-1_37}
}

@article{tomaz2020oblique,
	author    = {Tomaz Stepisnik and
	Dragi Kocev},
	title     = {Oblique Predictive Clustering Trees},
	journal   = {CoRR},
	volume    = {abs/2007.13617},
	year      = {2020},
	url       = {https://arxiv.org/abs/2007.13617},
	eprinttype = {arXiv},
	eprint    = {2007.13617},
	timestamp = {Wed, 29 Jul 2020 15:36:39 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2007-13617.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{guerra2008predicting,
	author = {Guerra, Silvio and Prudencio, Ricardo and Ludermir, Teresa},
	year = {2008},
	month = {09},
	pages = {523-532},
	title = {Predicting the Performance of Learning Algorithms Using Support Vector Machines as Meta-regressors},
	volume = {5163},
	isbn = {978-3-540-87535-2},
	doi = {10.1007/978-3-540-87536-9_54}
}

@article{reif2012automatic,
	author = {Reif, Matthias and Shafait, Faisal and Goldstein, Markus and Breuel, Thomas and Dengel, Andreas},
	year = {2012},
	month = {02},
	pages = {},
	title = {Automatic Classifier Selection for Non-Experts},
	volume = {17},
	journal = {Pattern Analysis and Applications},
	doi = {10.1007/s10044-012-0280-z}
}

@inproceedings{ridd2014using,
	author = {Ridd, Parker and Giraud-Carrier, Christophe},
	title = {Using Metalearning to Predict When Parameter Optimization is Likely to Improve Classification Accuracy},
	year = {2014},
	isbn = {16130073},
	publisher = {CEUR-WS.org},
	address = {Aachen, DEU},
	abstract = {Work on metalearning for algorithm selection has often been criticized because it
	mostly considers only the default parameter settings of the candidate base learning
	algorithms. Many have indeed argued that the choice of parameter values can have a
	significant impact on accuracy. Yet little empirical evidence exists to provide definitive
	support for that argument. Recent experiments do suggest that parameter optimization
	may indeed have an impact. However, the distribution of performance differences has
	a long tail, suggesting that in most cases parameter optimization has little effect
	on accuracy. In this paper, we revisit some of these results and use metalearning
	to characterize the situations when parameter optimization is likely to cause a significant
	increase in accuracy. In so doing, we show that 1) a relatively simple and efficient
	landmarker carries significant predictive power, and 2) metalearning for algorithm
	selection should be effected in two phases, the first in which one determines whether
	parameter optimization is likely to increase accuracy, and the second in which algorithm
	selection actually takes place.},
	booktitle = {Proceedings of the 2014 International Conference on Meta-Learning and Algorithm Selection - Volume 1201},
	pages = {18–23},
	numpages = {6},
	location = {Prague, Czech Republic},
	series = {MLAS'14}
}

@inproceedings{sanders2017informing,
	author = {Sanders, Samantha and Giraud-Carrier, Christophe},
	year = {2017},
	month = {11},
	pages = {1051-1056},
	title = {Informing the Use of Hyperparameter Optimization Through Metalearning},
	doi = {10.1109/ICDM.2017.137}
}

@inproceedings{leite2017selecting,
	author = {Leite, Rui and Brazdil, Pavel and Vanschoren, Joaquin},
	year = {2012},
	month = {07},
	pages = {117-131},
	title = {Selecting Classification Algorithms with Active Testing},
	volume = {7376},
	isbn = {978-3-642-31536-7},
	journal = {Lecture Notes in Computer Science},
	doi = {10.1007/978-3-642-31537-4_10}
}


@article{abdulrahman2018speeding,
	author = {Abdulrahman, Salisu Mamman and Brazdil, Pavel and Rijn, Jan N. and Vanschoren, Joaquin},
	title = {Speeding up Algorithm Selection Using Average Ranking and Active Testing by Introducing Runtime},
	year = {2018},
	issue_date = {January   2018},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = {107},
	number = {1},
	issn = {0885-6125},
	url = {https://doi.org/10.1007/s10994-017-5687-8},
	doi = {10.1007/s10994-017-5687-8},
	abstract = {Algorithm selection methods can be speeded-up substantially by incorporating multi-objective
	measures that give preference to algorithms that are both promising and fast to evaluate.
	In this paper, we introduce such a measure, A3R, and incorporate it into two algorithm
	selection techniques: average ranking and active testing. Average ranking combines
	algorithm rankings observed on prior datasets to identify the best algorithms for
	a new dataset. The aim of the second method is to iteratively select algorithms to
	be tested on the new dataset, learning from each new evaluation to intelligently select
	the next best candidate. We show how both methods can be upgraded to incorporate a
	multi-objective measure A3R that combines accuracy and runtime. It is necessary to
	establish the correct balance between accuracy and runtime, as otherwise time will
	be wasted by conducting less informative tests. The correct balance can be set by
	an appropriate parameter setting within function A3R that trades off accuracy and
	runtime. Our results demonstrate that the upgraded versions of Average Ranking and
	Active Testing lead to much better mean interval loss values than their accuracy-based
	counterparts.},
	journal = {Mach. Learn.},
	month = jan,
	pages = {79–108},
	numpages = {30},
	keywords = {Loss curves, Meta-learning, Average ranking, Algorithm selection, Mean interval loss, Ranking of algorithms, Active testing}
}

@inproceedings{witsuba2015learning,
	author={Wistuba, Martin and Schilling, Nicolas and Schmidt-Thieme, Lars},
	booktitle={2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)}, 
	title={Learning hyperparameter optimization initializations}, 
	year={2015},
	volume={},
	number={},
	pages={1-10},
	doi={10.1109/DSAA.2015.7344817}
}


@article{abdulrahman2014measures,
	author = {Abdulrahman, Salisu and Brazdil, Pavel},
	year = {2014},
	month = {01},
	pages = {49-50},
	title = {Measures for combining accuracy and time for meta-learning},
	volume = {1201},
	journal = {CEUR Workshop Proceedings}
}

@inproceedings{rijn2018hyp,
	author = {van Rijn, Jan N. and Hutter, Frank},
	title = {Hyperparameter Importance Across Datasets},
	year = {2018},
	isbn = {9781450355520},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3219819.3220058},
	doi = {10.1145/3219819.3220058},
	abstract = {With the advent of automated machine learning, automated hyperparameter optimization
	methods are by now routinely used in data mining. However, this progress is not yet
	matched by equal progress on automatic analyses that yield information beyond performance-optimizing
	hyperparameter settings. In this work, we aim to answer the following two questions:
	Given an algorithm, what are generally its most important hyperparameters, and what
	are typically good values for these? We present methodology and a framework to answer
	these questions based on meta-learning across many datasets. We apply this methodology
	using the experimental meta-data available on OpenML to determine the most important
	hyperparameters of support vector machines, random forests and Adaboost, and to infer
	priors for all their hyperparameters. The results, obtained fully automatically, provide
	a quantitative basis to focus efforts in both manual algorithm design and in automated
	hyperparameter optimization. The conducted experiments confirm that the hyperparameters
	selected by the proposed method are indeed the most important ones and that the obtained
	priors also lead to statistically significant improvements in hyperparameter optimization.},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery},
	pages = {2367–2376},
	numpages = {10},
	keywords = {meta-learning, hyperparameter importance, hyperparameter optimization},
	location = {London, United Kingdom},
	series = {KDD '18}
}


@inproceedings{witsuba2015hyper,
	author = {Wistuba, Martin and Schilling, Nicolas and Schmidt-Thieme, Lars},
	year = {2015},
	month = {09},
	pages = {104-119},
	title = {Hyperparameter Search Space Pruning - A New Component for Sequential Model-Based Hyperparameter Optimization},
	doi = {10.1007/978-3-319-23525-7_7}
}

@inproceedings{pinto2016towards,
	author = {F{\'a}bio Pinto and C. Soares and Jo{\~a}o Mendes-Moreira},
	year = {2016},
	month = {04},
	pages = {215-226},
	title = {Towards Automatic Generation of Metafeatures},
	volume = {9651},
	isbn = {978-3-319-31752-6},
	doi = {10.1007/978-3-319-31753-3_18}
}

@inproceedings{bardenet2013hyper,
	author = {Bardenet, R\'{e}mi and Brendel, M\'{a}ty\'{a}s and K\'{e}gl, Bal\'{a}zs and Sebag, Mich\`{e}le},
	title = {Collaborative Hyperparameter Tuning},
	year = {2013},
	publisher = {JMLR.org},
	abstract = {Hyperparameter learning has traditionally been a manual task because of the limited
	number of trials. Today's computing infrastructures allow bigger evaluation budgets,
	thus opening the way for algorithmic approaches. Recently, surrogate-based optimization
	was successfully applied to hyperparameter learning for deep belief networks and to
	WEKA classifiers. The methods combined brute force computational power with model
	building about the behavior of the error function in the hyperparameter space, and
	they could significantly improve on manual hyperparameter tuning. What may make experienced
	practitioners even better at hyperparameter optimization is their ability to generalize
	across similar learning problems. In this paper, we propose a generic method to incorporate
	knowledge from previous experiments when simultaneously tuning a learning algorithm
	on new problems at hand. To this end, we combine surrogate-based ranking and optimization
	techniques for surrogate-based collaborative tuning (SCoT). We demonstrate SCoT in
	two experiments where it outperforms standard tuning techniques and single-problem
	surrogate-based optimization.},
	booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	pages = {II–199–II–207},
	location = {Atlanta, GA, USA},
	series = {ICML'13}
}


@InProceedings{yogatama2014efficient,
	title = 	 {{Efficient Transfer Learning Method for Automatic Hyperparameter Tuning}},
	author = 	 {Yogatama, Dani and Mann, Gideon},
	booktitle = 	 {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {1077--1085},
	year = 	 {2014},
	editor = 	 {Kaski, Samuel and Corander, Jukka},
	volume = 	 {33},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Reykjavik, Iceland},
	month = 	 {22--25 Apr},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v33/yogatama14.pdf},
	url = 	 {https://proceedings.mlr.press/v33/yogatama14.html},
	abstract = 	 {We propose a fast and effective algorithm for automatic hyperparameter tuning that can generalize across datasets. Our method is an instance of sequential model-based optimization (SMBO) that transfers information by constructing a common response surface for all datasets, similar to Bardenet et al. (2013). The time complexity of reconstructing the response surface at every SMBO iteration in our method is linear in the number of trials (significantly less than previous work with comparable performance), allowing the method to realistically scale to many more datasets. Specifically, we use deviations from the per-dataset mean as the response values. We empirically show the superiority of our method on a large number of synthetic and real-world datasets for tuning hyperparameters of logistic regression and ensembles of classifiers.}
}

@inproceedings{schilling2015hyp,
	title={Hyperparameter optimization with factorized multilayer perceptrons},
	author={Schilling, Nicolas and Wistuba, Martin and Drumond, Lucas and Schmidt-Thieme, Lars},
	booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	pages={87--103},
	year={2015},
	organization={Springer}
}

@article{witsuba2018scalable,
	author = {Wistuba, Martin and Schilling, Nicolas and Schmidt-Thieme, Lars},
	year = {2018},
	month = {01},
	pages = {},
	title = {Scalable Gaussian process-based transfer surrogates for hyperparameter optimization},
	volume = {107},
	journal = {Machine Learning},
	doi = {10.1007/s10994-017-5684-y}
}

@misc{activmetal,
	TITLE = {{ActivMetaL: Algorithm Recommendation with Active Meta Learning}},
	AUTHOR = {Sun-Hosoya, Lisheng and Guyon, Isabelle and Sebag, Mich{\`e}le},
	URL = {https://hal.archives-ouvertes.fr/hal-01931262},
	NOTE = {Poster},
	HOWPUBLISHED = {{IAL 2018 workshop, ECML PKDD}},
	YEAR = {2018},
	MONTH = Sep,
	PDF = {https://hal.archives-ouvertes.fr/hal-01931262/file/ACTIVMETAL.pdf},
	HAL_ID = {hal-01931262},
	HAL_VERSION = {v1},
}

@article{hospedales2021metalearning,
	author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
	year = {2021},
	month = {05},
	pages = {1-1},
	title = {Meta-Learning in Neural Networks: A Survey},
	volume = {PP},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	doi = {10.1109/TPAMI.2021.3079209}
}

@article{lemke2013metalearning,
	author = {Lemke, Christiane and Budka, Marcin and Gabrys, Bogdan},
	year = {2013},
	month = {06},
	pages = {},
	title = {Metalearning: a survey of trends and technologies},
	volume = {DOI: 10.1007/s10462-013-9406-y},
	journal = {Artificial Intelligence Review},
	doi = {10.1007/s10462-013-9406-y}
}

@article{vilalta2001perspective,
	author = {Vilalta, Ricardo and Drissi, Youssef},
	year = {2001},
	month = {09},
	pages = {},
	title = {A Perspective View And Survey Of Meta-Learning},
	volume = {18},
	journal = {Artificial Intelligence Review},
	doi = {10.1023/A:1019956318069}
}

@inproceedings{giraud2008metalearning,
	title={Metalearning-a tutorial},
	author={Giraud-Carrier, Christophe},
	booktitle={Tutorial at the 7th international conference on machine learning and applications (ICMLA), San Diego, California, USA},
	year={2008}
}

@book{bradzil2009metalearning,
	author = {Brazdil, Pavel and Giraud-Carrier, Christophe and Soares, Carlos and Vilalta, Ricardo},
	year = {2009},
	month = {01},
	pages = {},
	title = {Metalearning - Applications to Data Mining.},
	isbn = {978-3-540-73262-4},
	journal = {Metalearning: Applications to Data Mining, Cognitive Technologies. ISBN 978-3-540-73262-4. Springer Berlin Heidelberg, 2009},
	doi = {10.1007/978-3-540-73263-1}
}

@phdthesis{vanschoren2010understanding,
	author = {Vanschoren, Joaquin},
	year = {2010},
	month = {05},
	pages = {},
	title = {Understanding Machine Learning Performance with Experiment Databases}
}

@techreport{wolpert1995no,
	title={No free lunch theorems for search},
	author={Wolpert, David H and Macready, William G and others},
	year={1995},
	institution={Technical Report SFI-TR-95-02-010, Santa Fe Institute}
}

@misc{bischl2021hyperparameter,
	title={Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges}, 
	author={Bernd Bischl and Martin Binder and Michel Lang and Tobias Pielok and Jakob Richter and Stefan Coors and Janek Thomas and Theresa Ullmann and Marc Becker and Anne-Laure Boulesteix and Difan Deng and Marius Lindauer},
	year={2021},
	eprint={2107.05847},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{scikit-learn,
	title={Scikit-learn: Machine Learning in {P}ython},
	author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
	and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
	and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
	Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	journal={Journal of Machine Learning Research},
	volume={12},
	pages={2825--2830},
	year={2011}
}

@inproceedings{wu2021frugal,
	title={Frugal optimization for cost-related hyperparameters},
	author={Wu, Qingyun and Wang, Chi and Huang, Silu},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={35},
	pages={10347--10354},
	year={2021}
}

@misc{wu2019practical,
	title={Practical Multi-fidelity Bayesian Optimization for Hyperparameter Tuning}, 
	author={Jian Wu and Saul Toscano-Palmerin and Peter I. Frazier and Andrew Gordon Wilson},
	year={2019},
	eprint={1903.04703},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{vanschoren2018metalearning,
	title={Meta-Learning: A Survey}, 
	author={Joaquin Vanschoren},
	year={2018},
	eprint={1810.03548},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{fuerer2015initializing,
	author = {Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
	title = {Initializing Bayesian Hyperparameter Optimization via Meta-Learning},
	year = {2015},
	isbn = {0262511290},
	publisher = {AAAI Press},
	abstract = {Model selection and hyperparameter optimization is crucial in applying machine learning
	to a novel dataset. Recently, a sub-community of machine learning has focused on solving
	this problem with Sequential Model-based Bayesian Optimization (SMBO), demonstrating
	substantial successes in many applications. However, for computationally expensive
	algorithms the overhead of hyperparameter optimization can still be prohibitive. In
	this paper we mimic a strategy human domain experts use: speed up optimization by
	starting from promising configurations that performed well on similar datasets. The
	resulting initialization technique integrates naturally into the generic SMBO framework
	and can be trivially applied to any SMBO method. To validate our approach, we perform
	extensive experiments with two established SMBO frameworks (Spearmint and SMAC) with
	complementary strengths; optimizing two machine learning frameworks on 57 datasets.
	Our initialization procedure yields mild improvements for low-dimensional hyperparameter
	optimization and substantially improves the state of the art for the more complex
	combined algorithm selection and hyperparameter optimization problem.},
	booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
	pages = {1128–1135},
	numpages = {8},
	location = {Austin, Texas},
	series = {AAAI'15}
}

@article{atomic,
	author = {Moniz, Nuno and Cerqueira, Vitor},
	year = {2021},
	month = {04},
	pages = {115011},
	title = {Automated Imbalanced Classification via Meta-learning},
	volume = {178},
	journal = {Expert Systems with Applications},
	doi = {10.1016/j.eswa.2021.115011}
}

@article{Feurer2020AutoSklearn2T,
	title={Auto-Sklearn 2.0: The Next Generation},
	author={Matthias Feurer and Katharina Eggensperger and S. Falkner and M. Lindauer and F. Hutter},
	journal={ArXiv},
	year={2020},
	volume={abs/2007.04074}
}

@article{borkowski1994optimal,
	author = {Borkowski, John and Pukelsheim, Friedrich},
	year = {1994},
	month = {05},
	pages = {214},
	title = {Optimal Design of Experiments},
	volume = {36},
	journal = {Technometrics},
	doi = {10.2307/1270234}
}

@inbook{makar2011teaching,
	author = {Makar, Katie and Fielding-Wells, Jill},
	year = {2011},
	month = {06},
	pages = {347-358},
	title = {Teaching Teachers to Teach Statistical Investigations},
	isbn = {978-94-007-1130-3},
	doi = {10.1007/978-94-007-1131-0_33}
}

@inproceedings{falkner2018bohb,
	title={BOHB: Robust and efficient hyperparameter optimization at scale},
	author={Falkner, Stefan and Klein, Aaron and Hutter, Frank},
	booktitle={International Conference on Machine Learning},
	pages={1437--1446},
	year={2018},
	organization={PMLR}
}

@article{li2018hyperband,
	author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
	title = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
	year = {2017},
	issue_date = {January 2017},
	publisher = {JMLR.org},
	volume = {18},
	number = {1},
	issn = {1532-4435},
	abstract = {Performance of machine learning algorithms depends critically on identifying a good
	set of hyperparameters. While recent approaches use Bayesian optimization to adaptively
	select configurations, we focus on speeding up random search through adaptive resource
	allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration
	nonstochastic infinite-armed bandit problem where a predefined resource like iterations,
	data samples, or features is allocated to randomly sampled configurations. We introduce
	a novel algorithm, Hyperband, for this framework and analyze its theoretical properties,
	providing several desirable guarantees. Furthermore, we compare Hyperband with popular
	Bayesian optimization methods on a suite of hyperparameter optimization problems.
	We observe that Hyperband can provide over an order-of-magnitude speedup over our
	competitor set on a variety of deep-learning and kernel-based learning problems.},
	journal = {J. Mach. Learn. Res.},
	month = jan,
	pages = {6765–6816},
	numpages = {52},
	keywords = {hyperparameter optimization, deep learning, model selection, online optimization, infinite-armed bandits}
}

@inproceedings{hutter2011sequential,
	title={Sequential model-based optimization for general algorithm configuration},
	author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
	booktitle={International conference on learning and intelligent optimization},
	pages={507--523},
	year={2011},
	organization={Springer}
}

@article{wolpert1992stacked,
	author = {Wolpert, David},
	year = {1992},
	month = {12},
	pages = {241-259},
	title = {Stacked Generalization},
	volume = {5},
	journal = {Neural Networks},
	doi = {10.1016/S0893-6080(05)80023-1}
}

@article{breiman1996stacked,
	author = {Breiman, Leo},
	title = {Stacked Regressions},
	year = {1996},
	issue_date = {July 1996},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = {24},
	number = {1},
	issn = {0885-6125},
	url = {https://doi.org/10.1023/A:1018046112532},
	doi = {10.1023/A:1018046112532},
	journal = {Mach. Learn.},
	month = jul,
	pages = {49–64},
	numpages = {16},
	keywords = {combinations, trees, stacking, non-negativity, subset regression}
}

@inproceedings{snoek2012practical,
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
	title = {Practical Bayesian Optimization of Machine Learning Algorithms},
	year = {2012},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning
	parameters and model hyperparameters. Unfortunately, this tuning is often a "black
	art" requiring expert experience, rules of thumb, or sometimes brute-force search.
	There is therefore great appeal for automatic approaches that can optimize the performance
	of any given learning algorithm to the problem at hand. In this work, we consider
	this problem through the framework of Bayesian optimization, in which a learning algorithm's
	generalization performance is modeled as a sample from a Gaussian process (GP). We
	show that certain choices for the nature of the GP, such as the type of kernel and
	the treatment of its hyperparameters, can play a crucial role in obtaining a good
	optimizer that can achieve expertlevel performance. We describe new algorithms that
	take into account the variable cost (duration) of learning algorithm experiments and
	that can leverage the presence of multiple cores for parallel experimentation. We
	show that these proposed algorithms improve on previous automatic procedures and can
	reach or surpass human expert-level optimization for many algorithms including latent
	Dirichlet allocation, structured SVMs and convolutional neural networks.},
	booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
	pages = {2951–2959},
	numpages = {9},
	location = {Lake Tahoe, Nevada},
	series = {NIPS'12}
}

@inproceedings{maclaurin2015gradient,
	author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P.},
	title = {Gradient-Based Hyperparameter Optimization through Reversible Learning},
	year = {2015},
	publisher = {JMLR.org},
	abstract = {Tuning hyperparameters of learning algorithms is hard because gradients are usually
	unavailable. We compute exact gradients of cross-validation performance with respect
	to all hyperparameters by chaining derivatives backwards through the entire training
	procedure. These gradients allow us to optimize thousands of hyperparameters, including
	step-size and momentum schedules, weight initialization distributions, richly parameterized
	regularization schemes, and neural network architectures. We compute hyperparameter
	gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
	pages = {2113–2122},
	numpages = {10},
	location = {Lille, France},
	series = {ICML'15}
}

@inproceedings{weimar2007cofirank,
	author = {Weimer, Markus and Karatzoglou, Alexandros and Le, Quoc Viet and Smola, Alex},
	title = {CofiRank Maximum Margin Matrix Factorization for Collaborative Ranking},
	year = {2007},
	isbn = {9781605603520},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {In this paper, we consider collaborative filtering as a ranking problem. We present
	a method which uses Maximum Margin Matrix Factorization and optimizes ranking instead
	of rating. We employ structured output prediction to optimize directly for ranking
	scores. Experimental results show that our method gives very good ranking scores and
	scales well on collaborative filtering tasks.},
	booktitle = {Proceedings of the 20th International Conference on Neural Information Processing Systems},
	pages = {1593–1600},
	numpages = {8},
	location = {Vancouver, British Columbia, Canada},
	series = {NIPS'07}
}

@techreport{burges2010from,
	author = {Burges, Chris J.C.},
	title = {From RankNet to LambdaRank to LambdaMART: An Overview},
	year = {2010},
	month = {June},
	abstract = {LambdaMART is the boosted tree version of LambdaRank, which is based on RankNet. RankNet, LambdaRank, and LambdaMART have proven to be very successful algorithms for solving real world ranking problems: for example an ensemble of LambdaMART rankers won Track 1 of the 2010 Yahoo! Learning To Rank Challenge. The details of these algorithms are spread across several papers and reports, and so here we give a self-contained, detailed and complete description of them.},
	url = {https://www.microsoft.com/en-us/research/publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/},
	number = {MSR-TR-2010-82},
}

@article{rankml,
	author    = {Doron Laadan and
	Roman Vainshtein and
	Yarden Curiel and
	Gilad Katz and
	Lior Rokach},
	title     = {RankML: a Meta Learning-Based Approach for Pre-Ranking Machine Learning
	Pipelines},
	journal   = {CoRR},
	volume    = {abs/1911.00108},
	year      = {2019},
	url       = {http://arxiv.org/abs/1911.00108},
	eprinttype = {arXiv},
	eprint    = {1911.00108},
	timestamp = {Sat, 23 Jan 2021 01:20:08 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1911-00108.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pge2015,
	title={A New Grammatical Evolution Based on Probabilistic Context-free Grammar},
	author={Hyun-Tae Kim and Chang Wook Ahn},
	year={2015},
	doi={10.1007/978-3-319-13356-0_1},
	url={https://doi.org/10.1007/978-3-319-13356-0_1},
	publisher = {Springer International Publishing},
	pages = {1--12},
	author = {Hyun-Tae Kim and Chang Wook Ahn},
	title = {A New Grammatical Evolution Based on Probabilistic Context-free Grammar},
	booktitle = {Proceedings in Adaptation, Learning and Optimization}
}

@InProceedings{castiello2005metadata,
	author="Castiello, Ciro
	and Castellano, Giovanna
	and Fanelli, Anna Maria",
	editor="Torra, Vicen{\c{c}}
	and Narukawa, Yasuo
	and Miyamoto, Sadaaki",
	title="Meta-data: Characterization of Input Features for Meta-learning",
	booktitle="Modeling Decisions for Artificial Intelligence",
	year="2005",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="457--468",
	abstract="Common inductive learning strategies offer the tools for knowledge acquisition, but possess some inherent limitations due to the use of fixed bias during the learning process. To overcome limitations of such base-learning approaches, a novel research trend is oriented to explore the potentialities of meta-learning, which is oriented to the development of mechanisms based on a dynamical search of bias. This could lead to an improvement of the base-learner performance on specific learning tasks, by profiting of the accumulated past experience. As a significant set of I/O data is needed for efficient base-learning, appropriate meta-data characterization is of crucial importance for useful meta-learning. In order to characterize meta-data, firstly a collection of meta-features discriminating among different base-level tasks should be identified. This paper focuses on the characterization of meta-data, through an analysis of meta-features that can capture the properties of specific tasks to be solved at base level. This kind of approach represents a first step toward the development of a meta-learning system, capable of suggesting the proper bias for base-learning different specific task domains.",
	isbn="978-3-540-31883-5"
}
